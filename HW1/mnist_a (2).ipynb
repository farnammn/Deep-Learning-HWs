{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "with stddev 0.2, epoch 0, Average loss : 2.075164391560988, Accuracy : 0.542473\n",
      "with stddev 0.2, epoch 5, Average loss : 1.6368500380082565, Accuracy : 0.822909\n",
      "with stddev 0.2, epoch 10, Average loss : 1.5992591586979952, Accuracy : 0.838509\n",
      "with stddev 0.2, epoch 15, Average loss : 1.5840850327231668, Accuracy : 0.846327\n",
      "with stddev 0.2, epoch 20, Average loss : 1.575557921366258, Accuracy : 0.850600\n",
      "with stddev 0.2, epoch 25, Average loss : 1.5700109481811524, Accuracy : 0.852455\n",
      "with stddev 0.2, epoch 30, Average loss : 1.5659259674765846, Accuracy : 0.853782\n",
      "with stddev 0.2, epoch 35, Average loss : 1.562600990642201, Accuracy : 0.851309\n",
      "with stddev 0.2, epoch 40, Average loss : 1.5586661176248031, Accuracy : 0.839236\n",
      "with stddev 0.2, epoch 45, Average loss : 1.5547406846826726, Accuracy : 0.839436\n",
      "with stddev 0.2, epoch 50, Average loss : 1.5518078637123107, Accuracy : 0.841055\n",
      "Total time: 106.12920188903809 seconds\n",
      "Optimization Finished!\n",
      "Accuracy stddev 0.2, 0.8356\n",
      "with stddev 2.0, epoch 0, Average loss : 2.0796407983519813, Accuracy : 0.500818\n",
      "with stddev 2.0, epoch 5, Average loss : 1.6449636281620372, Accuracy : 0.827545\n",
      "with stddev 2.0, epoch 10, Average loss : 1.604119235385548, Accuracy : 0.853873\n",
      "with stddev 2.0, epoch 15, Average loss : 1.5860879837382924, Accuracy : 0.864073\n",
      "with stddev 2.0, epoch 20, Average loss : 1.575939772129059, Accuracy : 0.873582\n",
      "with stddev 2.0, epoch 25, Average loss : 1.5690219768610867, Accuracy : 0.886327\n",
      "with stddev 2.0, epoch 30, Average loss : 1.5580504352396185, Accuracy : 0.913273\n",
      "with stddev 2.0, epoch 35, Average loss : 1.5527808117866515, Accuracy : 0.916382\n",
      "with stddev 2.0, epoch 40, Average loss : 1.549171681837602, Accuracy : 0.918145\n",
      "with stddev 2.0, epoch 45, Average loss : 1.5462081441012296, Accuracy : 0.920127\n",
      "with stddev 2.0, epoch 50, Average loss : 1.5437199319492687, Accuracy : 0.921109\n",
      "Total time: 106.74658465385437 seconds\n",
      "Optimization Finished!\n",
      "Accuracy stddev 2.0, 0.9153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nهمانطور که از نمودار مشخص است انگار با انحراف معیار 2\\nوزن ها کاهش می\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.5\n",
    "batch_size = 100\n",
    "n_epochs = 51\n",
    "delta = 1.0\n",
    "hidden_size = 10\n",
    "# regulation_rate = 1e-4\n",
    "\n",
    "def fullLayer(input_data, output_size, act, name, std = 0.1):\n",
    "    ##defining the full linear Layer here\n",
    "    w = tf.Variable(tf.random_normal([input_data.get_shape().as_list()[1], output_size], stddev = std, name = name + \"_weigth\"))\n",
    "#     w = tf.Variable(tf.zeros([input_data.shape[1], output_size]))\n",
    "    b = tf.Variable(tf.zeros([output_size]), name = name + \"_bias\")\n",
    "    return act(tf.matmul(input_data, w) + b), w, b\n",
    "\n",
    "#read data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "#define placeholder\n",
    "# all image are 28 * 28 so x has 784 dimension\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder')\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "stddev = tf.Variable(0.1, name = \"stddev\")\n",
    "\n",
    "## the layers\n",
    "h, w1, b1 = fullLayer(X, hidden_size, tf.nn.sigmoid, \"layer_1\", stddev)\n",
    "logits, w2, b2 = fullLayer(h, 10, tf.nn.sigmoid, \"layer_2\", stddev)\n",
    "\n",
    "## defining loss function\n",
    "## use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
    "## computes the mean over all the examples in the batch\n",
    "loss = tf.reduce_mean(entropy) \n",
    "# + regulation_rate*tf.nn.l2_loss(w)  \n",
    "\n",
    "##defining optimizer\n",
    "## using gradient descent with learning rate of 0.5 to minimize loss\n",
    "gradient = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "optimizer = gradient.minimize(loss)\n",
    "\n",
    "##the prediction we made\n",
    "preds = tf.nn.softmax(logits)\n",
    "##check how many of them are correct arg maxx is used because Y is one hat\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "#just some config for not getting whole server\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "\n",
    "with tf.Session(config = config) as sess:\n",
    "        \n",
    "    # to visualize using TensorBoard\n",
    "    ## making the graph of the network as b) demands\n",
    "#     writer = tf.summary.FileWriter('./graphs/mnist/a_2')\n",
    "    \n",
    "    for std in [0.2, 2.0]:\n",
    "        ##training the model \n",
    "        \n",
    "#         tf.summary.histogram(\"hidden_weigths\",tf.reduce_mean(w1))\n",
    "#         merge=tf.summary.merge_all()\n",
    "        ##starting time\n",
    "        start_time = time.time()\n",
    "        ##initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ##asign stddev\n",
    "        sess.run(stddev.assign(std))\n",
    "\n",
    "        \n",
    "        ## number of training batches\n",
    "        n_batches = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(n_epochs):  # train the model n_epochs times\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            for _ in range(n_batches):\n",
    "                ##training batches\n",
    "                X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "                _, loss_batch, acc_batch = sess.run([optimizer, loss, accuracy], feed_dict={X: X_batch, Y: Y_batch})\n",
    "                total_loss += loss_batch\n",
    "                total_acc += acc_batch\n",
    "                \n",
    "#                 writer.add_summary(w,std)\n",
    "            if i % 5 == 0:\n",
    "                print('with stddev {}, epoch {}, Average loss : {}, Accuracy : {:.6f}'.format(std, i, total_loss / n_batches, total_acc / mnist.train.num_examples))\n",
    "\n",
    "        print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "        print('Optimization Finished!')\n",
    "        \n",
    "        # test the model\n",
    "        ##number of test batches\n",
    "        n_batches = int(mnist.test.num_examples / batch_size)\n",
    "        total_correct_preds = 0\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            ##test batches\n",
    "            X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "            accuracy_batch = sess.run([accuracy], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            total_correct_preds += accuracy_batch[0]\n",
    "\n",
    "        print('Accuracy stddev {}, {}'.format(std, total_correct_preds / mnist.test.num_examples))\n",
    "        \n",
    "#     writer.close()\n",
    "sess.close()\n",
    "\n",
    "'''\n",
    "همانطور که از نمودار مشخص است انگار با انحراف معیار 2 وزن ها در حال کاهش یافتن هستند\n",
    "اما وقتی انحراف معیار 0.2 است این اتفاق نمی افتد\n",
    "می دانیم که خروجی سیگموید نزدیکی 0 شبیه خظی است اما در فاصله های دور تر به یک میل میکند\n",
    "پس اگر وزن ها زیادی بزرگ یا کوچک باشند جواب هاب کم تری را شبکه میتواند بپوشاند\n",
    "پس در فرآیند یادگیری در حالتی که وزن ها زیاداند کاهش میابند\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
