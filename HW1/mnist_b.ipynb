{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "with batch_size 1, epoch 0, Average loss : 1.6082455511461604, Accuracy : 0.820436\n",
      "Total time: 127.64351558685303 seconds\n",
      "Optimization Finished!\n",
      "Accuracy batch_size 1, 0.8787\n",
      "with batch_size 10, epoch 0, Average loss : 1.7089301073551177, Accuracy : 0.796891\n",
      "with batch_size 10, epoch 1, Average loss : 1.5909484843124042, Accuracy : 0.817455\n",
      "with batch_size 10, epoch 2, Average loss : 1.5739139389558272, Accuracy : 0.826382\n",
      "with batch_size 10, epoch 3, Average loss : 1.5657847174947912, Accuracy : 0.843491\n",
      "with batch_size 10, epoch 4, Average loss : 1.5607086818001488, Accuracy : 0.867145\n",
      "with batch_size 10, epoch 5, Average loss : 1.5573890530846335, Accuracy : 0.895727\n",
      "with batch_size 10, epoch 6, Average loss : 1.5545648713978855, Accuracy : 0.904927\n",
      "with batch_size 10, epoch 7, Average loss : 1.552099504925988, Accuracy : 0.909873\n",
      "with batch_size 10, epoch 8, Average loss : 1.5503066819797862, Accuracy : 0.912164\n",
      "with batch_size 10, epoch 9, Average loss : 1.546208662336523, Accuracy : 0.912873\n",
      "Total time: 133.48701310157776 seconds\n",
      "Optimization Finished!\n",
      "Accuracy batch_size 10, 0.9106\n",
      "with batch_size 50, epoch 0, Average loss : 1.9470973613045433, Accuracy : 0.618764\n",
      "with batch_size 50, epoch 5, Average loss : 1.6005523752082478, Accuracy : 0.853855\n",
      "with batch_size 50, epoch 10, Average loss : 1.57215390866453, Accuracy : 0.832036\n",
      "with batch_size 50, epoch 15, Average loss : 1.5610860367254777, Accuracy : 0.841418\n",
      "with batch_size 50, epoch 20, Average loss : 1.5547696907953783, Accuracy : 0.851782\n",
      "with batch_size 50, epoch 25, Average loss : 1.5502744622664018, Accuracy : 0.869200\n",
      "with batch_size 50, epoch 30, Average loss : 1.5458986964009025, Accuracy : 0.897364\n",
      "with batch_size 50, epoch 35, Average loss : 1.5409251604296945, Accuracy : 0.913182\n",
      "with batch_size 50, epoch 40, Average loss : 1.5375353531403975, Accuracy : 0.919036\n",
      "with batch_size 50, epoch 45, Average loss : 1.5349722947857596, Accuracy : 0.922327\n",
      "Total time: 135.7180199623108 seconds\n",
      "Optimization Finished!\n",
      "Accuracy batch_size 50, 0.9165\n",
      "with batch_size 100, epoch 0, Average loss : 2.07347511399876, Accuracy : 0.554073\n",
      "with batch_size 100, epoch 10, Average loss : 1.5944202225858515, Accuracy : 0.895236\n",
      "with batch_size 100, epoch 20, Average loss : 1.5619610084186901, Accuracy : 0.909691\n",
      "with batch_size 100, epoch 30, Average loss : 1.5508336509357799, Accuracy : 0.914545\n",
      "with batch_size 100, epoch 40, Average loss : 1.5445829569209706, Accuracy : 0.919018\n",
      "with batch_size 100, epoch 50, Average loss : 1.5402669954299926, Accuracy : 0.921436\n",
      "with batch_size 100, epoch 60, Average loss : 1.5370161552862687, Accuracy : 0.923109\n",
      "with batch_size 100, epoch 70, Average loss : 1.5344654967568137, Accuracy : 0.924945\n",
      "with batch_size 100, epoch 80, Average loss : 1.5323850438811562, Accuracy : 0.926273\n",
      "with batch_size 100, epoch 90, Average loss : 1.530614972114563, Accuracy : 0.926982\n",
      "Total time: 152.23468112945557 seconds\n",
      "Optimization Finished!\n",
      "Accuracy batch_size 100, 0.921\n",
      "with batch_size 1000, epoch 0, Average loss : 2.2919616959311746, Accuracy : 0.212418\n",
      "with batch_size 1000, epoch 100, Average loss : 1.596835108236833, Accuracy : 0.819600\n",
      "with batch_size 1000, epoch 200, Average loss : 1.5715651013634422, Accuracy : 0.834782\n",
      "with batch_size 1000, epoch 300, Average loss : 1.5605148987336592, Accuracy : 0.840418\n",
      "with batch_size 1000, epoch 400, Average loss : 1.5542384949597445, Accuracy : 0.844055\n",
      "with batch_size 1000, epoch 500, Average loss : 1.5499502051960339, Accuracy : 0.848200\n",
      "with batch_size 1000, epoch 600, Average loss : 1.546675809946927, Accuracy : 0.852964\n",
      "with batch_size 1000, epoch 700, Average loss : 1.5440385840155861, Accuracy : 0.858564\n",
      "with batch_size 1000, epoch 800, Average loss : 1.5418286410245028, Accuracy : 0.866709\n",
      "with batch_size 1000, epoch 900, Average loss : 1.5399622071873058, Accuracy : 0.874709\n",
      "Total time: 494.78331685066223 seconds\n",
      "Optimization Finished!\n",
      "Accuracy batch_size 1000, 0.8717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nمقدار لاس به صورت میانگین برای 1 کم تر است اما برای 10 و 50 و 100 تقریبا مشابه عمل می کند\\nو برای روش بچ کامل نیز چون تعدا ایپاک ها زیاد است زیاد میانگین لاس فرق نمیکند\\nاما توجه داشته باشید چون تعداد ایپاک ها را سایز بچ در نظر گرفتیم هر چه بچ سایز بزرگ تر بوده زمان بیشتری یادگیری طول می کشد\\nاما زمان یادگیری برای سایز های 1 تا 100 چون روی جی پی یو است زیاد تفاوت نمیکند اما برای 1000 خیلی بیشتر طول میکشد\\nنتیجه گیری کلی یکی این است که 10 و 50 و 100 بهتر عمل میککند\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.5\n",
    "n_epochs = 10\n",
    "hidden_size = 10\n",
    "# regulation_rate = 1e-4\n",
    "\n",
    "def fullLayer(input_data, output_size, act, std = 0.1):\n",
    "    \n",
    "    ##defining the full linear Layer here\n",
    "    w = tf.Variable(tf.random_normal([input_data.get_shape().as_list()[1], output_size], stddev = std))\n",
    "#     w = tf.Variable(tf.zeros([input_data.shape[1], output_size]))\n",
    "    b = tf.Variable(tf.zeros([output_size]))\n",
    "    return act(tf.matmul(input_data, w) + b)\n",
    "\n",
    "#read data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "\n",
    "\n",
    "for batch_size in [1, 10, 50, 100, 1000]:\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #define placeholder\n",
    "    # all image are 28 * 28 so x has 784 dimension\n",
    "    X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder')\n",
    "    Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "    ## the layers\n",
    "    h = fullLayer(X, hidden_size, tf.nn.sigmoid)\n",
    "    logits = fullLayer(h, 10, tf.nn.sigmoid)\n",
    "\n",
    "    ## defining loss function\n",
    "    ## use cross entropy of softmax of logits as the loss function\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
    "    ## computes the mean over all the examples in the batch\n",
    "    loss = tf.reduce_mean(entropy) \n",
    "    # + regulation_rate*tf.nn.l2_loss(w)  \n",
    "\n",
    "    ##defining optimizer\n",
    "    ## using gradient descent with learning rate of 0.5 to minimize loss\n",
    "    gradient = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    optimizer = gradient.minimize(loss)\n",
    "\n",
    "    ##the prediction we made\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    ##check how many of them are correct arg maxx is used because Y is one hat\n",
    "    correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "    \n",
    "    #just some config for not getting whole server\n",
    "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "    config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "    \n",
    "    with tf.Session(config = config) as sess:\n",
    "        \n",
    "        # to visualize using TensorBoard\n",
    "        tf.summary.scalar(\"loss__batch__size__\" + str(batch_size), loss)\n",
    "        merge=tf.summary.merge_all()\n",
    "        writer=tf.summary.FileWriter(\"./graphs/mnist/b_all\")\n",
    "\n",
    "        ##starting time\n",
    "        start_time = time.time()\n",
    "        ##initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        ##number of train batches\n",
    "        n_batches = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(batch_size):  \n",
    "            ##because no matter what batch_size vector computations take similar time \n",
    "            ##it is logical that number of epochs must be linearly dependant to batch_size\n",
    "            ##so if number of epoches be batch_size all of batch_sizes proccesses can take similar time\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            for _ in range(n_batches):\n",
    "                ##train batches\n",
    "                X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "                \n",
    "                _, loss_batch, acc_batch = sess.run([optimizer, loss, accuracy], feed_dict={X: X_batch, Y: Y_batch})\n",
    "                total_loss += loss_batch\n",
    "                total_acc += acc_batch\n",
    "\n",
    "            if i % max(batch_size // 10, 1) == 0:\n",
    "                print('with batch_size {}, epoch {}, Average loss : {}, Accuracy : {:.6f}'.format(batch_size, i, total_loss / n_batches, total_acc / mnist.train.num_examples))\n",
    "\n",
    "        print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "        print('Optimization Finished!')  # should be around 0.35 after 25 epochs\n",
    "\n",
    "        ## test the model\n",
    "        ## number of test batches\n",
    "        n_batches = int(mnist.test.num_examples / batch_size)\n",
    "        total_correct_preds = 0\n",
    "        for i in range(n_batches):\n",
    "            ##test batches\n",
    "            X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "            accuracy_batch, m = sess.run([accuracy, merge], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            total_correct_preds += accuracy_batch\n",
    "            \n",
    "            writer.add_summary(m, i)\n",
    "\n",
    "        print('Accuracy batch_size {}, {}'.format(batch_size, total_correct_preds / mnist.test.num_examples))\n",
    "\n",
    "        writer.close()\n",
    "    sess.close()\n",
    "\n",
    "'''\n",
    "وقتی سایز بچ 1 است مقدار گرادیان محاسبه شده تقریب بسیار ضعیفی از گرادیان است از این رو \n",
    "مقدار لاس به صورت میانگین برای 1 خیلی کم تر است اما برای 10 و 50 و 100 تقریبا مشابه عمل می کند\n",
    "و برای روش بچ کامل نیز چون تعداد ایپاک ها زیاد است زیاد میانگین لاس فرق نمیکند\n",
    "اما توجه داشته باشید چون تعداد ایپاک ها را سایز بچ در نظر گرفتیم هر چه بچ سایز بزرگ تر بوده زمان بیشتری یادگیری طول می کشد\n",
    "اما زمان یادگیری برای سایز های 1 تا 100 چون روی جی پی یو است زیاد تفاوت نمیکنند\n",
    "اما برای بچ سایز1000 با اینکه تعداد بار هایی که گرادیان را محاسبه میکنیم یکسان است خیلی بیشتر طول میکشد\n",
    "بیشتر از سه برابر بیشتر طول میکشد\n",
    "توجه کنید که مهم بهتر عمل کردن در زمان کم تر است\n",
    "نتیجه گیری کلی یکی این است که 10 و 50 و 100 بهتر عمل میکنند\n",
    "'''   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
