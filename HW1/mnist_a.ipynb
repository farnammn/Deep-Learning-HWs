{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "with stddev 0.1, epoch 0, Average loss : 2.072131069356745, Accuracy : 0.523636\n",
      "with stddev 0.1, epoch 5, Average loss : 1.645405415838415, Accuracy : 0.862400\n",
      "with stddev 0.1, epoch 10, Average loss : 1.602555796883323, Accuracy : 0.884364\n",
      "with stddev 0.1, epoch 15, Average loss : 1.5817704046856274, Accuracy : 0.899200\n",
      "with stddev 0.1, epoch 20, Average loss : 1.5690109560706398, Accuracy : 0.905255\n",
      "with stddev 0.1, epoch 25, Average loss : 1.5613020446083763, Accuracy : 0.908691\n",
      "with stddev 0.1, epoch 30, Average loss : 1.5559762300144542, Accuracy : 0.911545\n",
      "with stddev 0.1, epoch 35, Average loss : 1.5520813885602085, Accuracy : 0.913509\n",
      "with stddev 0.1, epoch 40, Average loss : 1.5489126764644277, Accuracy : 0.915000\n",
      "with stddev 0.1, epoch 45, Average loss : 1.5463550183989785, Accuracy : 0.916982\n",
      "with stddev 0.1, epoch 50, Average loss : 1.5441779093308883, Accuracy : 0.918255\n",
      "Total time: 114.00116038322449 seconds\n",
      "Optimization Finished!\n",
      "Accuracy stddev 0.1, 0.9131\n",
      "with stddev 0.3, epoch 0, Average loss : 2.0794068056886847, Accuracy : 0.549364\n",
      "with stddev 0.3, epoch 5, Average loss : 1.6479667975685812, Accuracy : 0.784200\n",
      "with stddev 0.3, epoch 10, Average loss : 1.6011084671453997, Accuracy : 0.821345\n",
      "with stddev 0.3, epoch 15, Average loss : 1.5823556691950018, Accuracy : 0.834945\n",
      "with stddev 0.3, epoch 20, Average loss : 1.5725880102677778, Accuracy : 0.853073\n",
      "with stddev 0.3, epoch 25, Average loss : 1.566194288513877, Accuracy : 0.880345\n",
      "with stddev 0.3, epoch 30, Average loss : 1.5591739747741005, Accuracy : 0.906255\n",
      "with stddev 0.3, epoch 35, Average loss : 1.551042267625982, Accuracy : 0.916618\n",
      "with stddev 0.3, epoch 40, Average loss : 1.547196906263178, Accuracy : 0.919309\n",
      "with stddev 0.3, epoch 45, Average loss : 1.5442617011070252, Accuracy : 0.920291\n",
      "with stddev 0.3, epoch 50, Average loss : 1.5419817582043736, Accuracy : 0.921764\n",
      "Total time: 113.73953294754028 seconds\n",
      "Optimization Finished!\n",
      "Accuracy stddev 0.3, 0.9148\n",
      "with stddev 1.0, epoch 0, Average loss : 2.069876648946242, Accuracy : 0.515473\n",
      "with stddev 1.0, epoch 5, Average loss : 1.6390485115484759, Accuracy : 0.865582\n",
      "with stddev 1.0, epoch 10, Average loss : 1.598988015651703, Accuracy : 0.889455\n",
      "with stddev 1.0, epoch 15, Average loss : 1.5789781858704306, Accuracy : 0.902545\n",
      "with stddev 1.0, epoch 20, Average loss : 1.5667505374821749, Accuracy : 0.909945\n",
      "with stddev 1.0, epoch 25, Average loss : 1.5589217964085667, Accuracy : 0.913400\n",
      "with stddev 1.0, epoch 30, Average loss : 1.5533553218841554, Accuracy : 0.916400\n",
      "with stddev 1.0, epoch 35, Average loss : 1.5490877496112476, Accuracy : 0.918436\n",
      "with stddev 1.0, epoch 40, Average loss : 1.54571528933265, Accuracy : 0.920818\n",
      "with stddev 1.0, epoch 45, Average loss : 1.542810464772311, Accuracy : 0.922636\n",
      "with stddev 1.0, epoch 50, Average loss : 1.5404053330421448, Accuracy : 0.923509\n",
      "Total time: 114.58107018470764 seconds\n",
      "Optimization Finished!\n",
      "Accuracy stddev 1.0, 0.9183\n",
      "with stddev 2.0, epoch 0, Average loss : 2.092096730145541, Accuracy : 0.480018\n",
      "with stddev 2.0, epoch 5, Average loss : 1.6366087339141153, Accuracy : 0.824236\n",
      "with stddev 2.0, epoch 10, Average loss : 1.5943428089401939, Accuracy : 0.834236\n",
      "with stddev 2.0, epoch 15, Average loss : 1.5788529107787392, Accuracy : 0.837036\n",
      "with stddev 2.0, epoch 20, Average loss : 1.5689080769365484, Accuracy : 0.832073\n",
      "with stddev 2.0, epoch 25, Average loss : 1.5618936445496299, Accuracy : 0.833818\n",
      "with stddev 2.0, epoch 30, Average loss : 1.556765505400571, Accuracy : 0.835545\n",
      "with stddev 2.0, epoch 35, Average loss : 1.5529809479279952, Accuracy : 0.837673\n",
      "with stddev 2.0, epoch 40, Average loss : 1.550068505894054, Accuracy : 0.839909\n",
      "with stddev 2.0, epoch 45, Average loss : 1.547735442681746, Accuracy : 0.841436\n",
      "with stddev 2.0, epoch 50, Average loss : 1.5457467950474133, Accuracy : 0.843891\n",
      "Total time: 111.33851313591003 seconds\n",
      "Optimization Finished!\n",
      "Accuracy stddev 2.0, 0.8364\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.5\n",
    "batch_size = 100\n",
    "n_epochs = 51\n",
    "delta = 1.0\n",
    "hidden_size = 10\n",
    "# regulation_rate = 1e-4\n",
    "\n",
    "def fullLayer(input_data, output_size, act, name, std = 0.1):\n",
    "    ##defining the full linear Layer here\n",
    "    w = tf.Variable(tf.random_normal([input_data.get_shape().as_list()[1], output_size], stddev = std, name = name + \"_weigth\"))\n",
    "#     w = tf.Variable(tf.zeros([input_data.shape[1], output_size]))\n",
    "    b = tf.Variable(tf.zeros([output_size]), name = name + \"_bias\")\n",
    "    return act(tf.matmul(input_data, w) + b)\n",
    "\n",
    "#read data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "#define placeholder\n",
    "# all image are 28 * 28 so x has 784 dimension\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder')\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "stddev = tf.Variable(0.1, name = \"stddev\")\n",
    "\n",
    "## the layers\n",
    "h = fullLayer(X, hidden_size, tf.nn.sigmoid, \"layer_1\", stddev)\n",
    "logits = fullLayer(h, 10, tf.nn.sigmoid, \"layer_2\", stddev)\n",
    "\n",
    "## defining loss function\n",
    "## use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
    "## computes the mean over all the examples in the batch\n",
    "loss = tf.reduce_mean(entropy) \n",
    "# + regulation_rate*tf.nn.l2_loss(w)  \n",
    "\n",
    "##defining optimizer\n",
    "## using gradient descent with learning rate of 0.5 to minimize loss\n",
    "gradient = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "optimizer = gradient.minimize(loss)\n",
    "\n",
    "##the prediction we made\n",
    "preds = tf.nn.softmax(logits)\n",
    "##check how many of them are correct arg maxx is used because Y is one hat\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "#just some config for not getting whole server\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "\n",
    "with tf.Session(config = config) as sess:\n",
    "        \n",
    "    # to visualize using TensorBoard\n",
    "    ## making the graph of the network as b) demands\n",
    "    writer = tf.summary.FileWriter('./graphs/mnist/a', sess.graph)\n",
    "    for std in [0.1, 0.3, 1.0, 2.0]:\n",
    "        ##training the model \n",
    "        \n",
    "        ##starting time\n",
    "        start_time = time.time()\n",
    "        ##initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ##assign stddev\n",
    "        sess.run(stddev.assign(std))\n",
    "        \n",
    "        \n",
    "        ## number of training batches\n",
    "        n_batches = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(n_epochs):  # train the model n_epochs times\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            for _ in range(n_batches):\n",
    "                ##training batches\n",
    "                X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "                _, loss_batch, acc_batch = sess.run([optimizer, loss, accuracy], feed_dict={X: X_batch, Y: Y_batch})\n",
    "                total_loss += loss_batch\n",
    "                total_acc += acc_batch\n",
    "            if i % 5 == 0:\n",
    "                print('with stddev {}, epoch {}, Average loss : {}, Accuracy : {:.6f}'.format(std, i, total_loss / n_batches, total_acc / mnist.train.num_examples))\n",
    "\n",
    "        print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "        print('Optimization Finished!')\n",
    "        \n",
    "        # test the model\n",
    "        ##number of test batches\n",
    "        n_batches = int(mnist.test.num_examples / batch_size)\n",
    "        total_correct_preds = 0\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            ##test batches\n",
    "            X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "            accuracy_batch = sess.run([accuracy], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            total_correct_preds += accuracy_batch[0]\n",
    "\n",
    "        print('Accuracy stddev {}, {}'.format(std, total_correct_preds / mnist.test.num_examples))\n",
    "\n",
    "    writer.close()\n",
    "sess.close()\n",
    "\n",
    "'''\n",
    "همانظور که میبینید مقدار واریانس وزن ها تاثیر زیادی در مقدار تابع خطا و اکیوریسی ندارد\n",
    "حقیقتا این اتفاق دور از آن چیزی بود که انتظارش را داشتم زیرا وقتی واریانس وزن ها زیاد باشد احتمالا وزن های بزرگ بیشتر خواهد بود و ورودی هر نود میتواند عدد بزرگتری بگیرد\n",
    "و در نتیجه مشتق تابع خظا نسبت به وزن های متصل شده به هر نود ضرب در مشتق سیگموید ورودی آن نود میشود\n",
    "و از آن جا که مشتق سیگموید در مقادیر بسیار بزرگ و کوچک به صورت نمایی مقدارش کم میشود\n",
    "پس مشتق تابع خطا نسبت به این وزن ها نیز بسیار کوچک خواهد بود پس\n",
    "این وزن ها تغییر زیادی نخواهند کرد\n",
    "البته با انحراف معیار 2 اکیوریسی به مقدار قابل توجهی کاهش یافته که شاهد این ماجرا است\n",
    "البته لرنینگ ریت ما بزرگ است شاید این باعث شده که تاثیر این ماجرا کم تر بشود\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
