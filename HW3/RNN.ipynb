{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-27 23:07:29: Loss after num_examples_seen=2000 epoch=0: 0.637180\n",
      "2017-11-27 23:07:31: Loss after num_examples_seen=4000 epoch=1: 0.478966\n",
      "2017-11-27 23:07:32: Loss after num_examples_seen=6000 epoch=2: 0.461109\n",
      "2017-11-27 23:07:33: Loss after num_examples_seen=8000 epoch=3: 0.376358\n",
      "2017-11-27 23:07:34: Loss after num_examples_seen=10000 epoch=4: 0.333396\n",
      "2017-11-27 23:07:36: Loss after num_examples_seen=12000 epoch=5: 0.363458\n",
      "2017-11-27 23:07:37: Loss after num_examples_seen=14000 epoch=6: 0.529089\n",
      "2017-11-27 23:07:38: Loss after num_examples_seen=16000 epoch=7: 0.566999\n",
      "2017-11-27 23:07:39: Loss after num_examples_seen=18000 epoch=8: 0.538437\n",
      "2017-11-27 23:07:41: Loss after num_examples_seen=20000 epoch=9: 0.538166\n",
      "2017-11-27 23:07:42: Loss after num_examples_seen=22000 epoch=10: 0.539357\n",
      "2017-11-27 23:07:43: Loss after num_examples_seen=24000 epoch=11: 0.539117\n",
      "2017-11-27 23:07:44: Loss after num_examples_seen=26000 epoch=12: 0.538956\n",
      "2017-11-27 23:07:45: Loss after num_examples_seen=28000 epoch=13: 0.538777\n",
      "2017-11-27 23:07:47: Loss after num_examples_seen=30000 epoch=14: 0.538466\n",
      "2017-11-27 23:07:48: Loss after num_examples_seen=32000 epoch=15: 0.537101\n",
      "2017-11-27 23:07:49: Loss after num_examples_seen=34000 epoch=16: 0.533409\n",
      "2017-11-27 23:07:51: Loss after num_examples_seen=36000 epoch=17: 0.532493\n",
      "2017-11-27 23:07:52: Loss after num_examples_seen=38000 epoch=18: 0.503433\n",
      "2017-11-27 23:07:53: Loss after num_examples_seen=40000 epoch=19: 0.452237\n",
      "2017-11-27 23:07:54: Loss after num_examples_seen=42000 epoch=20: 0.433355\n",
      "2017-11-27 23:07:55: Loss after num_examples_seen=44000 epoch=21: 0.432114\n",
      "2017-11-27 23:07:57: Loss after num_examples_seen=46000 epoch=22: 0.431484\n",
      "2017-11-27 23:07:58: Loss after num_examples_seen=48000 epoch=23: 0.431086\n",
      "2017-11-27 23:07:59: Loss after num_examples_seen=50000 epoch=24: 0.430809\n",
      "2017-11-27 23:08:00: Loss after num_examples_seen=52000 epoch=25: 0.430604\n",
      "2017-11-27 23:08:02: Loss after num_examples_seen=54000 epoch=26: 0.430445\n",
      "2017-11-27 23:08:03: Loss after num_examples_seen=56000 epoch=27: 0.430319\n",
      "2017-11-27 23:08:04: Loss after num_examples_seen=58000 epoch=28: 0.430215\n",
      "2017-11-27 23:08:05: Loss after num_examples_seen=60000 epoch=29: 0.430129\n",
      "2017-11-27 23:08:07: Loss after num_examples_seen=62000 epoch=30: 0.430056\n",
      "2017-11-27 23:08:08: Loss after num_examples_seen=64000 epoch=31: 0.429994\n",
      "2017-11-27 23:08:09: Loss after num_examples_seen=66000 epoch=32: 0.429939\n",
      "2017-11-27 23:08:10: Loss after num_examples_seen=68000 epoch=33: 0.429892\n",
      "2017-11-27 23:08:11: Loss after num_examples_seen=70000 epoch=34: 0.429850\n",
      "2017-11-27 23:08:13: Loss after num_examples_seen=72000 epoch=35: 0.429812\n",
      "2017-11-27 23:08:14: Loss after num_examples_seen=74000 epoch=36: 0.429779\n",
      "2017-11-27 23:08:15: Loss after num_examples_seen=76000 epoch=37: 0.429748\n",
      "2017-11-27 23:08:17: Loss after num_examples_seen=78000 epoch=38: 0.429721\n",
      "2017-11-27 23:08:18: Loss after num_examples_seen=80000 epoch=39: 0.429696\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27f867bc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0\n",
      "  0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0\n",
      "  1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0\n",
      "  0 1 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1\n",
      "  0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 0 1\n",
      "  1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0\n",
      "  0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1\n",
      "  0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0\n",
      "  1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt #matplotlib inline\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, word_dim, hidden_dim=2, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, word_dim))\n",
    "        #To implement\n",
    "        self.V = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (word_dim, hidden_dim)) # check\n",
    "        self.W = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.dU = []\n",
    "        self.dV = []\n",
    "        self.dW = []\n",
    "\n",
    "\n",
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        # To implement : Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        s[t]= np.tanh( self.U.dot(x[t]) + self.W.dot(s[t-1])) \n",
    "        o[t] = softmax( self.V.dot(s[t]) )\n",
    "    \n",
    "    return [o, s]\n",
    "RNN.forward_propagation = forward_propagation\n",
    "\n",
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "RNN.predict = predict\n",
    "\n",
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # For each sequence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "\n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x, y) / N\n",
    "RNN.calculate_total_loss = calculate_total_loss\n",
    "RNN.calculate_loss = calculate_loss\n",
    "\n",
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        # To Implement\n",
    "        dLdV += np.outer(delta_o[t], s[t])\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2)) #derivate wrt Ws + Ux\n",
    "        # To implement: Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t - self.bptt_truncate), t + 1)[::-1]:\n",
    "            dLdW += np.outer(delta_t, s[bptt_step - 1])\n",
    "            dLdU += np.outer(delta_t, x[bptt_step])\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step - 1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "RNN.bptt = bptt\n",
    "\n",
    "# Performs one step of SGD.\n",
    "def numpy_sgd_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    #To implement: Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "    self.dU.append(dLdU)\n",
    "    self.dV.append(dLdV)\n",
    "    self.dW.append(dLdW)\n",
    "RNN.sgd_step = numpy_sgd_step\n",
    "\n",
    "\n",
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.5, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        total_loss = 0\n",
    "        for x , y in zip(X_train, y_train):\n",
    "            # Optionally evaluate the loss\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = model.calculate_loss(x, y)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                total_loss += loss\n",
    "                \n",
    "#                 # Adjust the learning rate if loss increases\n",
    "#                 if (len(losses) > 1 and losses[-1][1] >losses[-2][1]):\n",
    "#                     learning_rate = learning_rate * 0.5\n",
    "#                     print(\"Setting learning rate to %f\" % learning_rate)\n",
    "                sys.stdout.flush()\n",
    "            # For each training example...\n",
    "            for i in range(len(y)):\n",
    "                # One SGD step\n",
    "                model.sgd_step(x[i], y[i], learning_rate)\n",
    "                num_examples_seen += 1\n",
    "        time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, total_loss / len(X_train)))\n",
    "\n",
    "def generate_y(data_in, len_seq = 5):\n",
    "    data_out = []\n",
    "    for d in data_in:\n",
    "        a = 0\n",
    "        for i in d:\n",
    "            a = (a + i[1]) % 2\n",
    "            data_out.append(a)\n",
    "    return np.array(data_out).reshape(-1, len_seq)\n",
    "\n",
    "def generate_x(string, len_seq = 5):\n",
    "    data_in = [[int(i == '0'), int(i == '1')] for i in string]\n",
    "    return np.array(data_in).reshape(-1, len_seq, 2)\n",
    "\n",
    "\n",
    "model = RNN(2)\n",
    "\n",
    "len_seq = 5\n",
    "len_train = 10000\n",
    "batch_size = 10\n",
    "\n",
    "data_in = [str(random.randint(0, 1)) for i in range(len_train)]\n",
    "x = generate_x(data_in)\n",
    "y = generate_y(x)\n",
    "\n",
    "x = x.reshape(-1, batch_size, len_seq, 2)\n",
    "y = y.reshape(-1, batch_size, len_seq)\n",
    "losses = train_with_sgd(model,x, y,learning_rate = 0.01, nepoch=40, evaluate_loss_after=1)\n",
    "plt.clf()\n",
    "plt.plot(model.dU[0][0], color='red', label = 'dU through time')\n",
    "plt.show()\n",
    "\n",
    "f = open(\"Q1.in\", 'r')\n",
    "data_in = f.readlines()[0]\n",
    "f.close()\n",
    "\n",
    "x_test = generate_x(data_in, len_seq = len(data_in))\n",
    "\n",
    "# x_test=np.asarray([[[int(w == 0), int(w == 1)]] for w in X_seq])\n",
    "print(generate_y(x_test, len_seq = len(data_in)))\n",
    "for x in x_test:\n",
    "    predictions = model.predict(x)\n",
    "    f = open(\"Q1.out\", 'w')\n",
    "    s = \"\"\n",
    "    for p in predictions:\n",
    "        s += str(p)\n",
    "    f.write(s)\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
