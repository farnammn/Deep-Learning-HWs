{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "class Layer(object):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, inputs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, inputs, layer_err):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_grad(self, inputs, layer_err):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, inputs, layer_err, lr):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class ConvolutionalLayer(Layer):\n",
    "    def __init__(self, filters, strides):\n",
    "        self.filters = filters\n",
    "        self.strides = strides\n",
    "        \n",
    "    def forward( self, inputs ):\n",
    "        h_filter, w_filter, d1, d2 = self.filters.shape\n",
    "        batch_size, h, w, d1 = inputs.shape\n",
    "\n",
    "        output = np.zeros((batch_size, int( (h - h_filter) / self.strides[0] ) + 1 , int( (w - w_filter) / self.strides[1] ) + 1 , d2))\n",
    "\n",
    "        for i in range(0, h - h_filter + 1,self.strides[0]):\n",
    "            for j in range(0, w - w_filter + 1, self.strides[1]):\n",
    "                output[:,i // self.strides[0],  j // self.strides[1], : ] = np.sum(inputs[:, i :i + h_filter, j : j + w_filter, :, np.newaxis] * self.filters, axis=(1, 2, 3))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, inputs, layer_err):\n",
    "        h_filter, w_filter, d1, d2 = self.filters.shape\n",
    "        batch_size, h, w, d2 = layer_err.shape\n",
    "\n",
    "        d_output_h = (h - 1) * self.strides[0] + h_filter\n",
    "        d_output_w = (w - 1) * self.strides[1] + w_filter\n",
    "\n",
    "        d_output = np.zeros((batch_size, d_output_h, d_output_w, d1))\n",
    "\n",
    "        for i in range(0 , d_output_h - h_filter + 1, self.strides[0]): \n",
    "            for j in range(0, d_output_w - w_filter+ 1, self.strides[1]):\n",
    "                d_output[:, i:i + h_filter, j:j + w_filter, :] += np.sum(self.filters[np.newaxis, ...] * layer_err[:, int(i / self.strides[0]) : int(i / self.strides[0]) + 1, int(j / self.strides[1]) : int(j / self.strides[1]) + 1, np.newaxis, :], axis=4)\n",
    "        return d_output\n",
    "\n",
    "    def get_grad(self, inputs, layer_err):\n",
    "                \n",
    "        total_layer_err = np.sum(layer_err, axis=(0, 1, 2))\n",
    "        filters_err = np.zeros(self.filters.shape)\n",
    "\n",
    "        s1 = (inputs.shape[1] - self.filters.shape[0]) // self.strides[0] + 1\n",
    "        s2 = (inputs.shape[2] - self.filters.shape[1]) // self.strides[1] + 1\n",
    "\n",
    "        summed_inputs = np.sum(inputs, axis=0)\n",
    "\n",
    "        for i in range(s1):\n",
    "            for j in range(s2):\n",
    "                filters_err += summed_inputs[i  * self.strides[0]:i * self.strides[0] + self.filters.shape[0], j * self.strides[1]:j * self.strides[1] + self.filters.shape[1], :, np.newaxis]\n",
    "        return filters_err * total_layer_err\n",
    "    def update(self, inputs, layer_err, lr):\n",
    "        self.filters -= self.get_grad(inputs, layer_err) * lr\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MaxPool(Layer):\n",
    "    \n",
    "    def __init__(self, ksizes, strides):\n",
    "        self.ksizes = ksizes\n",
    "        self.strides = strides\n",
    "        \n",
    "    def forward(self,  inputs ):\n",
    "        batch_size, h, w, d = inputs.shape\n",
    "\n",
    "        output = np.zeros((batch_size, int( (h - self.ksizes[0]) / self.strides[0] ) + 1,int( (w - self.ksizes[1]) / self.strides[1] ) + 1 , d))\n",
    "\n",
    "        for i in range(0, h - self.ksizes[0] + 1, self.strides[0]):\n",
    "            for j in range(0, w - self.ksizes[1] + 1, self.strides[1]):\n",
    "                output[:,i // self.strides[0],  j // self.strides[1], : ] = np.amax(inputs[:, i :i + self.ksizes[0], j : j + self.ksizes[1], :] , axis=(1, 2))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def to_one_hat(self, y, d):\n",
    "        o = np.zeros((len(y), self.ksizes[0], self.ksizes[1], d))\n",
    "        for i in range(len(y)):\n",
    "            o[i, y[i] // (self.ksizes[1] * d), (y[i]//d) % self.ksizes[1], y % d] = 1\n",
    "        return o\n",
    "    \n",
    "    def backward(self, inputs, layer_err):\n",
    "        batch_size, h2, w2, d = layer_err.shape\n",
    "\n",
    "        h = (h2 -1) * self.strides[0] + self.ksizes[0]\n",
    "        w = (w2 -1) * self.strides[1] + self.ksizes[1]\n",
    "\n",
    "        output = np.zeros((batch_size, h, w , d))\n",
    "\n",
    "        for i in range(0, h + 1 - self.ksizes[0], self.strides[0]):\n",
    "            for j in range(0, w + 1 - self.ksizes[1], self.strides[1]):\n",
    "                \n",
    "                output[:, i:i + self.ksizes[0], j:j + self.ksizes[1], :] += layer_err[:, i // self.strides[0]:i // self.strides[0] + 1 , j // self.strides[1]:j // self.strides[1]+1 ,:] * self.to_one_hat(np.argmax(inputs[:, i :i + self.ksizes[0], j : j + self.ksizes[1], :].reshape(batch_size, -1) ,axis = 1), d)\n",
    "                \n",
    "        return output\n",
    "    def update(self, inputs, layer_err, lr):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 25, 25, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpool = MaxPool((3,3), (2,2))\n",
    "inputs = np.random.rand(50, 25,25 , 3)\n",
    "# print(np.argmax(inputs, axis = 0).shape)\n",
    "layer_err = np.random.rand(50, 12, 12, 3)\n",
    "# print(mpool.forward(inputs))\n",
    "mpool.backward(inputs, layer_err).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FullyConnected(Layer):\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return inputs.dot(self.w) + self.b\n",
    "\n",
    "    def backward(self, inputs, layer_err):\n",
    "        return layer_err.dot(self.w.T)\n",
    "\n",
    "    def get_grad(self, inputs, layer_err):\n",
    "        return inputs.T.dot(layer_err)\n",
    "    \n",
    "    def update(self, inputs, layer_err, lr):\n",
    "        self.w -= self.get_grad(inputs, layer_err) * lr\n",
    "        self.b -= np.sum(layer_err, axis = 0) * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Flatten(Layer):\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return np.reshape(inputs, (inputs.shape[0], -1))\n",
    "\n",
    "    def backward(self, inputs, layer_err):\n",
    "        return np.reshape(layer_err, (inputs.shape))\n",
    "\n",
    "    def get_grad(self, inputs, layer_err):\n",
    "        return 0.\n",
    "\n",
    "    def update(self, inputs, layer_err, lr):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    @abstractmethod\n",
    "    def out(self,  x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivetive(self, x):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Relu(Activation):\n",
    "    def out(self, x):\n",
    "    #Rectified Linear Unit at x\n",
    "        return (x+abs(x))/2.0\n",
    "    def derivetive(self, x):\n",
    "    #derivation of Rectified Linear Unit at x\n",
    "        return (x>0)*1.0\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def out(self, x):\n",
    "    #sigmoid function at x (element wise)\n",
    "    #input and output is a numpy matrix with size of [nx1]\n",
    "        ### YOUR CODE HERE:\n",
    "        return 1. / ( 1. + np.exp(-x))\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def derivetive(self, x):\n",
    "    #derivation of sigmoid function at x\n",
    "        tmp=self.out(x)\n",
    "        return tmp * (1. - tmp)\n",
    "    \n",
    "class Identity(Activation):\n",
    "    def out(self, x):\n",
    "        return x\n",
    "    def derivetive(self, x):\n",
    "        return 1.0\n",
    "\n",
    "def softmax_grad(s):\n",
    "    out = np.diag(s)\n",
    "    print(out.shape)\n",
    "\n",
    "    for i in range(len(out)):\n",
    "        for j in range(len(out)):\n",
    "            if i == j:\n",
    "                \n",
    "                out[i][j] = s[i] * (1-s[i])\n",
    "            else: \n",
    "                out[i][j] = -s[i]*s[j]\n",
    "    return out\n",
    "\n",
    "def softmax(x): \n",
    "#softmax function at x\n",
    "#input and output is a numpy matrix with size of [nx1]\n",
    "    ### YOUR CODE HERE:\n",
    "    softmax_offset = np.max(x)\n",
    "    o = np.exp(x - softmax_offset) / np.sum(np.exp(x - softmax_offset))\n",
    "    return o\n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def softmax_ce_derivation(y_hat,y):\n",
    "#derivation of CE(softmax(y),y_hat) respect to y\n",
    "\treturn (softmax(y_hat)-y) / y_hat.shape[0]\n",
    "\n",
    "\n",
    "def ce_error(y_hat_normalized,y):\n",
    "#Cross Entropy error :CE(y,y_hat)\n",
    "\treturn \t-1.0*np.multiply(y,np.log(y_hat_normalized+1.0e-20)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "r = Relu()\n",
    "print(r.derivetive(np.array([  1,-1, 0,0.00004])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25, 25, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ 5139.11107802],\n",
       "         [ 5651.71394924],\n",
       "         [ 5438.9726763 ]],\n",
       "\n",
       "        [[ 5771.73652983],\n",
       "         [ 5612.54261032],\n",
       "         [ 5771.08384927]],\n",
       "\n",
       "        [[ 5127.4364122 ],\n",
       "         [ 5791.44669269],\n",
       "         [ 5424.28880883]]],\n",
       "\n",
       "\n",
       "       [[[ 4728.50124347],\n",
       "         [ 5011.61884923],\n",
       "         [ 5269.93092797]],\n",
       "\n",
       "        [[ 5446.41691579],\n",
       "         [ 5362.7878687 ],\n",
       "         [ 5125.73773597]],\n",
       "\n",
       "        [[ 4790.54577415],\n",
       "         [ 5075.71883381],\n",
       "         [ 5343.13744101]]],\n",
       "\n",
       "\n",
       "       [[[ 5123.19774618],\n",
       "         [ 5844.19437744],\n",
       "         [ 5541.39379867]],\n",
       "\n",
       "        [[ 5867.1362964 ],\n",
       "         [ 5669.72743163],\n",
       "         [ 5724.5418821 ]],\n",
       "\n",
       "        [[ 5139.4665904 ],\n",
       "         [ 6010.32094448],\n",
       "         [ 5521.6493788 ]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters = np.random.rand(3, 3,3 , 1)\n",
    "conv = ConvolutionalLayer(filters, (2,2))\n",
    "inputs = np.random.rand(1, 25, 25, 3)\n",
    "\n",
    "layer_err = np.random.rand(1, 12, 12, 1)\n",
    "# print(conv.forward(inputs))\n",
    "print(conv.backward( inputs, layer_err).shape)\n",
    "conv.get_grad(inputs, layer_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#read data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with epoch 0, Average loss : 309.30990369261406, Accuracy : 0.413527\n",
      "with epoch 1, Average loss : 309.99359259267527, Accuracy : 0.507709\n",
      "with epoch 2, Average loss : 310.1712651087862, Accuracy : 0.523164\n",
      "with epoch 3, Average loss : 310.2647350356926, Accuracy : 0.534309\n",
      "with epoch 4, Average loss : 310.32151049134393, Accuracy : 0.537727\n",
      "with epoch 5, Average loss : 310.3636285708555, Accuracy : 0.544873\n",
      "with epoch 6, Average loss : 310.3927588706572, Accuracy : 0.545400\n",
      "with epoch 7, Average loss : 310.41486517635224, Accuracy : 0.544145\n",
      "with epoch 8, Average loss : 310.436417058474, Accuracy : 0.544364\n",
      "with epoch 9, Average loss : 310.45067154513856, Accuracy : 0.547818\n",
      "with epoch 10, Average loss : 310.46654507438734, Accuracy : 0.545655\n",
      "with epoch 11, Average loss : 310.4751476648913, Accuracy : 0.551964\n",
      "with epoch 12, Average loss : 310.48838656879246, Accuracy : 0.548400\n",
      "with epoch 13, Average loss : 310.49409435813, Accuracy : 0.548582\n",
      "with epoch 14, Average loss : 310.50384364545533, Accuracy : 0.548436\n",
      "with epoch 15, Average loss : 310.5105425995295, Accuracy : 0.550145\n",
      "with epoch 16, Average loss : 310.5162981080085, Accuracy : 0.551018\n",
      "with epoch 17, Average loss : 310.5227573627128, Accuracy : 0.546982\n",
      "with epoch 18, Average loss : 310.52634118790763, Accuracy : 0.551273\n",
      "with epoch 19, Average loss : 310.5323886815418, Accuracy : 0.549382\n",
      "with epoch 20, Average loss : 310.5359796829521, Accuracy : 0.550582\n",
      "with epoch 21, Average loss : 310.54103520573506, Accuracy : 0.550055\n",
      "with epoch 22, Average loss : 310.5442911593467, Accuracy : 0.547455\n",
      "with epoch 23, Average loss : 310.546257497464, Accuracy : 0.552927\n",
      "with epoch 24, Average loss : 310.5506090081004, Accuracy : 0.550164\n",
      "with epoch 25, Average loss : 310.5536227683291, Accuracy : 0.548509\n",
      "with epoch 26, Average loss : 310.5557185462745, Accuracy : 0.550709\n",
      "with epoch 27, Average loss : 310.55851131425226, Accuracy : 0.548636\n",
      "with epoch 28, Average loss : 310.56164495492897, Accuracy : 0.548545\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-920e6029b6bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m#         X_batch = np.transpose(X_batch, [0, 2, 3, 1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtotal_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-920e6029b6bc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X_batch, Y_batch)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdE_dz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdE_dO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderivetive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mdE_dO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdE_dz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdE_dz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e2376868062a>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, inputs, layer_err)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_epochs = 100\n",
    "learning_rate = 0.05\n",
    "\n",
    "conv_filter_size = 4\n",
    "\n",
    "conv_stride_size = 1\n",
    "conv_layers = 64\n",
    "\n",
    "pool_filter_size = 3\n",
    "pool_stride_size = 2\n",
    "\n",
    "fc1_size = 512\n",
    "\n",
    "# filters = np.random.rand(conv_filter_size, conv_filter_size, 1, 3)\n",
    "# conv = ConvolutionalLayer(filters, (conv_stride_size , conv_stride_size))\n",
    "# mpool = MaxPool((pool_filter_size, pool_filter_size), (pool_stride_size,pool_stride_size))\n",
    "# w = np.random.rand(144 * 3, 10)\n",
    "# b = np.random.rand(10)\n",
    "# fc = FullyConnected(w, b)\n",
    "# layers_len = 4\n",
    "# layers = [conv, mpool,Flatten(), fc]\n",
    "# activations = [Sigmoid(), Sigmoid(), Identity(), Identity()]\n",
    "\n",
    "\n",
    "w = np.random.normal(0,0.1,(784,10))\n",
    "b = np.random.normal(0, 0.1, 10)\n",
    "\n",
    "fc1 = FullyConnected(w, b)\n",
    "\n",
    "w2 = np.random.normal(0,0.1,(10, 10))\n",
    "b2 = np.random.normal(0, 0.1, 10)\n",
    "fc2 = FullyConnected(w2, b2)\n",
    "\n",
    "layers_len = 1\n",
    "layers = [fc1, fc2]\n",
    "activations = [Sigmoid(), Relu()]\n",
    "\n",
    "\n",
    "## number of training batches\n",
    "n_batches = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "def train(X_batch, Y_batch):\n",
    "    os = []\n",
    "    zs = []\n",
    "    os.append(X_batch)\n",
    "    for i in range(layers_len):\n",
    "        zs.append(layers[i].forward(os[i]))\n",
    "        os.append(activations[i].out(zs[i]))\n",
    "\n",
    "    dE_dO = softmax_ce_derivation(os[layers_len], Y_batch) \n",
    "    \n",
    "    loss = ce_error(softmax(os[layers_len]), Y_batch)\n",
    "    correct_preds = np.argmax(os[layers_len], axis = 1) == np.argmax(Y_batch, axis = 1)\n",
    "    accuracy = np.sum(correct_preds) / float(len(X_batch))\n",
    "\n",
    "    for i in range(layers_len - 1, -1, -1):\n",
    "    \n",
    "        dE_dz = dE_dO * activations[i].derivetive(zs[i])\n",
    "        dE_dO = layers[i].backward(os[i], dE_dz)\n",
    "        layers[i].update(os[i], dE_dz, learning_rate)\n",
    "\n",
    "        \n",
    "    return loss, accuracy\n",
    "for i in range(100):  # train the model n_epochs times\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    \n",
    "    for _ in range(n_batches):\n",
    "        ##training batches\n",
    "        X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\n",
    "#         X_batch = X_batch.reshape(batch_size,1, 28, 28)\n",
    "#         X_batch = np.transpose(X_batch, [0, 2, 3, 1])\n",
    "        \n",
    "        loss, acc = train(X_batch, Y_batch)    \n",
    "        total_loss += loss\n",
    "        total_acc += acc\n",
    "#         print(acc)\n",
    "#         print(loss)\n",
    "    \n",
    "    print('with epoch {}, Average loss : {}, Accuracy : {:.6f}'.format(i, total_loss / n_batches, total_acc / n_batches))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
