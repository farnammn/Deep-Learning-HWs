{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (10000, 784)\n",
      "mean 33.7912244898\n",
      "max 255\n",
      "min 0\n",
      "shape (60000, 784)\n",
      "mean 33.3184214498\n",
      "max 255\n",
      "min 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from load_mnist import MNIST\n",
    "\n",
    "train_images, train_labels=MNIST(path=\"../Ehsan/Dataset/HW2/Q5\",return_type=\"numpy\",mode=\"vanilla\").load_training()\n",
    "test_images, test_labels=MNIST(path=\"../Ehsan/Dataset/HW2/Q5\",return_type=\"numpy\",mode=\"vanilla\").load_testing()\n",
    "\n",
    "for data in [test_images,train_images]:\n",
    "    print(\"shape\",data.shape)\n",
    "    print(\"mean\",data.mean())\n",
    "    print(\"max\",data.max())\n",
    "    print(\"min\",data.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c87a00d7434f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_one_hat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_one_hat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c87a00d7434f>\u001b[0m in \u001b[0;36mto_one_hat\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "def to_one_hat(y):\n",
    "    o = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        o[i][y[i] - 1] = 1\n",
    "    return o\n",
    "\n",
    "test_labels = to_one_hat(test_labels)\n",
    "train_labels = to_one_hat(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x = (x.T - np.mean(x, axis = 1)).T\n",
    "    x = (x.T / np.std(x, axis = 1)).T\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images = train_images.astype(float)\n",
    "test_images = test_images.astype(float)\n",
    "\n",
    "train_images = normalize(train_images)\n",
    "\n",
    "test_images = normalize(test_images)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 199, with std_dev 0.1, Average loss : 1.7617998731136322, Accuracy : 0.744333\n",
      "step 399, with std_dev 0.1, Average loss : 1.5854084551334382, Accuracy : 0.892000\n",
      "step 599, with std_dev 0.1, Average loss : 1.5573463553190232, Accuracy : 0.910167\n",
      "step 799, with std_dev 0.1, Average loss : 1.5394835656881332, Accuracy : 0.924667\n",
      "step 999, with std_dev 0.1, Average loss : 1.52846699655056, Accuracy : 0.933417\n",
      "step 1199, with std_dev 0.1, Average loss : 1.5204041415452958, Accuracy : 0.941000\n",
      "step 1399, with std_dev 0.1, Average loss : 1.5152849197387694, Accuracy : 0.945000\n",
      "step 1599, with std_dev 0.1, Average loss : 1.5104277181625365, Accuracy : 0.948917\n",
      "step 1799, with std_dev 0.1, Average loss : 1.5079844695329667, Accuracy : 0.951083\n",
      "step 1999, with std_dev 0.1, Average loss : 1.5050714641809464, Accuracy : 0.954917\n",
      "step 2199, with std_dev 0.1, Average loss : 1.5003749334812164, Accuracy : 0.958667\n",
      "step 2399, with std_dev 0.1, Average loss : 1.4986405688524247, Accuracy : 0.961333\n",
      "step 2599, with std_dev 0.1, Average loss : 1.4982794457674027, Accuracy : 0.960917\n",
      "step 2799, with std_dev 0.1, Average loss : 1.4957080686092377, Accuracy : 0.962583\n",
      "step 2999, with std_dev 0.1, Average loss : 1.4962287110090255, Accuracy : 0.962417\n",
      "step 3199, with std_dev 0.1, Average loss : 1.4907937347888947, Accuracy : 0.968833\n",
      "step 3399, with std_dev 0.1, Average loss : 1.4917491644620895, Accuracy : 0.967917\n",
      "step 3599, with std_dev 0.1, Average loss : 1.4896899712085725, Accuracy : 0.969417\n",
      "step 3799, with std_dev 0.1, Average loss : 1.489588854908943, Accuracy : 0.970583\n",
      "step 3999, with std_dev 0.1, Average loss : 1.4882455188035966, Accuracy : 0.971917\n",
      "step 4199, with std_dev 0.1, Average loss : 1.4849817329645156, Accuracy : 0.974917\n",
      "step 4399, with std_dev 0.1, Average loss : 1.4841877299547195, Accuracy : 0.976667\n",
      "step 4599, with std_dev 0.1, Average loss : 1.4857255852222442, Accuracy : 0.972917\n",
      "step 4799, with std_dev 0.1, Average loss : 1.4867171293497086, Accuracy : 0.973167\n",
      "step 4999, with std_dev 0.1, Average loss : 1.4847427368164063, Accuracy : 0.975333\n",
      "Total time: 30.79588532447815 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9636, with std_dev 0.1\n",
      "step 199, with std_dev 1, Average loss : 2.29945170044899, Accuracy : 0.108500\n",
      "step 399, with std_dev 1, Average loss : 2.230424019098282, Accuracy : 0.171250\n",
      "step 599, with std_dev 1, Average loss : 1.9758937633037568, Accuracy : 0.422667\n",
      "step 799, with std_dev 1, Average loss : 1.8200682342052459, Accuracy : 0.587667\n",
      "step 999, with std_dev 1, Average loss : 1.7505699217319488, Accuracy : 0.671750\n",
      "step 1199, with std_dev 1, Average loss : 1.706244444847107, Accuracy : 0.726083\n",
      "step 1399, with std_dev 1, Average loss : 1.6826105642318725, Accuracy : 0.755750\n",
      "step 1599, with std_dev 1, Average loss : 1.664466215968132, Accuracy : 0.774750\n",
      "step 1799, with std_dev 1, Average loss : 1.647921371459961, Accuracy : 0.798583\n",
      "step 1999, with std_dev 1, Average loss : 1.6359229159355164, Accuracy : 0.810583\n",
      "step 2199, with std_dev 1, Average loss : 1.63264828145504, Accuracy : 0.815500\n",
      "step 2399, with std_dev 1, Average loss : 1.6174268406629562, Accuracy : 0.828083\n",
      "step 2599, with std_dev 1, Average loss : 1.6184477424621582, Accuracy : 0.834250\n",
      "step 2799, with std_dev 1, Average loss : 1.6067122173309327, Accuracy : 0.842417\n",
      "step 2999, with std_dev 1, Average loss : 1.6055108869075776, Accuracy : 0.846583\n",
      "step 3199, with std_dev 1, Average loss : 1.6019766539335252, Accuracy : 0.849583\n",
      "step 3399, with std_dev 1, Average loss : 1.5956754243373872, Accuracy : 0.858333\n",
      "step 3599, with std_dev 1, Average loss : 1.5930151969194413, Accuracy : 0.859083\n",
      "step 3799, with std_dev 1, Average loss : 1.5917462491989136, Accuracy : 0.860167\n",
      "step 3999, with std_dev 1, Average loss : 1.5868087488412856, Accuracy : 0.866083\n",
      "step 4199, with std_dev 1, Average loss : 1.586964602470398, Accuracy : 0.865333\n",
      "step 4399, with std_dev 1, Average loss : 1.5842876213788986, Accuracy : 0.870500\n",
      "step 4599, with std_dev 1, Average loss : 1.5807600039243699, Accuracy : 0.872000\n",
      "step 4799, with std_dev 1, Average loss : 1.5751107281446457, Accuracy : 0.880500\n",
      "step 4999, with std_dev 1, Average loss : 1.5753920650482178, Accuracy : 0.878000\n",
      "Total time: 28.14297866821289 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.8753, with std_dev 1\n",
      "step 199, with std_dev 10, Average loss : 2.30528270483017, Accuracy : 0.086750\n",
      "step 399, with std_dev 10, Average loss : 2.297525259256363, Accuracy : 0.082333\n",
      "step 599, with std_dev 10, Average loss : 2.293397970199585, Accuracy : 0.080083\n",
      "step 799, with std_dev 10, Average loss : 2.293019745349884, Accuracy : 0.083667\n",
      "step 999, with std_dev 10, Average loss : 2.2943166601657867, Accuracy : 0.084667\n",
      "step 1199, with std_dev 10, Average loss : 2.2900999176502226, Accuracy : 0.087167\n",
      "step 1399, with std_dev 10, Average loss : 2.288066747188568, Accuracy : 0.089750\n",
      "step 1599, with std_dev 10, Average loss : 2.289197510480881, Accuracy : 0.089000\n",
      "step 1799, with std_dev 10, Average loss : 2.2876475727558137, Accuracy : 0.089167\n",
      "step 1999, with std_dev 10, Average loss : 2.2858488249778746, Accuracy : 0.088250\n",
      "step 2199, with std_dev 10, Average loss : 2.2868737316131593, Accuracy : 0.090750\n",
      "step 2399, with std_dev 10, Average loss : 2.2852741467952726, Accuracy : 0.089917\n",
      "step 2599, with std_dev 10, Average loss : 2.2855969107151033, Accuracy : 0.093250\n",
      "step 2799, with std_dev 10, Average loss : 2.285768737792969, Accuracy : 0.092667\n",
      "step 2999, with std_dev 10, Average loss : 2.2805714797973633, Accuracy : 0.096667\n",
      "step 3199, with std_dev 10, Average loss : 2.285064949989319, Accuracy : 0.095000\n",
      "step 3399, with std_dev 10, Average loss : 2.2815738999843598, Accuracy : 0.091583\n",
      "step 3599, with std_dev 10, Average loss : 2.2832028925418855, Accuracy : 0.096167\n",
      "step 3799, with std_dev 10, Average loss : 2.2795055150985717, Accuracy : 0.094167\n",
      "step 3999, with std_dev 10, Average loss : 2.279582591056824, Accuracy : 0.100583\n",
      "step 4199, with std_dev 10, Average loss : 2.277342630624771, Accuracy : 0.095667\n",
      "step 4399, with std_dev 10, Average loss : 2.278806742429733, Accuracy : 0.096000\n",
      "step 4599, with std_dev 10, Average loss : 2.282241396903992, Accuracy : 0.098000\n",
      "step 4799, with std_dev 10, Average loss : 2.2800562024116515, Accuracy : 0.098750\n",
      "step 4999, with std_dev 10, Average loss : 2.2732076454162597, Accuracy : 0.104583\n",
      "Total time: 34.28184723854065 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.1002, with std_dev 10\n",
      "step 199, with std_dev 20, Average loss : 2.3053323364257814, Accuracy : 0.068583\n",
      "step 399, with std_dev 20, Average loss : 2.3022550535202027, Accuracy : 0.071667\n",
      "step 599, with std_dev 20, Average loss : 2.302495254278183, Accuracy : 0.073750\n",
      "step 799, with std_dev 20, Average loss : 2.3022596192359925, Accuracy : 0.069750\n",
      "step 999, with std_dev 20, Average loss : 2.302626647949219, Accuracy : 0.070917\n",
      "step 1199, with std_dev 20, Average loss : 2.3027430176734924, Accuracy : 0.069500\n",
      "step 1399, with std_dev 20, Average loss : 2.3025782215595245, Accuracy : 0.070250\n",
      "step 1599, with std_dev 20, Average loss : 2.3021545803546903, Accuracy : 0.075500\n",
      "step 1799, with std_dev 20, Average loss : 2.3023112630844116, Accuracy : 0.073417\n",
      "step 1999, with std_dev 20, Average loss : 2.3022457754611967, Accuracy : 0.070333\n",
      "step 2199, with std_dev 20, Average loss : 2.3016232788562774, Accuracy : 0.076750\n",
      "step 2399, with std_dev 20, Average loss : 2.302511073350906, Accuracy : 0.071917\n",
      "step 2599, with std_dev 20, Average loss : 2.302531521320343, Accuracy : 0.071083\n",
      "step 2799, with std_dev 20, Average loss : 2.3024157762527464, Accuracy : 0.073167\n",
      "step 2999, with std_dev 20, Average loss : 2.3024329173564912, Accuracy : 0.071500\n",
      "step 3199, with std_dev 20, Average loss : 2.30241561293602, Accuracy : 0.072250\n",
      "step 3399, with std_dev 20, Average loss : 2.3022261726856232, Accuracy : 0.076083\n",
      "step 3599, with std_dev 20, Average loss : 2.3019387423992157, Accuracy : 0.072583\n",
      "step 3799, with std_dev 20, Average loss : 2.302584376335144, Accuracy : 0.068750\n",
      "step 3999, with std_dev 20, Average loss : 2.3022998642921446, Accuracy : 0.076750\n",
      "step 4199, with std_dev 20, Average loss : 2.3022062861919403, Accuracy : 0.074417\n",
      "step 4399, with std_dev 20, Average loss : 2.3024930858612063, Accuracy : 0.071833\n",
      "step 4599, with std_dev 20, Average loss : 2.301862416267395, Accuracy : 0.074833\n",
      "step 4799, with std_dev 20, Average loss : 2.302344559431076, Accuracy : 0.072750\n",
      "step 4999, with std_dev 20, Average loss : 2.3023703944683076, Accuracy : 0.078167\n",
      "Total time: 27.36003041267395 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.0762, with std_dev 20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "## Define paramaters for the model\n",
    "learning_rate = 0.05\n",
    "n_epochs = 5\n",
    "batch_size = 60\n",
    "hidden_size = 100\n",
    "learning_rate = 0.5\n",
    "\n",
    "# regulation_rate = 1e-4\n",
    "\n",
    "def normalize_batch(x , mu, ra_sigma):\n",
    "    x = tf.transpose((tf.transpose(x) - tf.reduce_mean(x, axis = 1)))\n",
    "    x = tf.transpose((tf.transpose(x) / tf.contrib.keras.backend.std(x, axis = 1)))\n",
    "    return  tf.scalar_mul(ra_sigma, x) + mu\n",
    "    \n",
    "def normalizedFullLayer(input_data, output_size, act, std = 0.1):    \n",
    "    ##defining the full linear Layer here\n",
    "    w = tf.Variable(tf.random_normal([input_data.get_shape().as_list()[1], output_size], stddev = std))\n",
    "    b = tf.Variable(tf.zeros([output_size]))\n",
    "    a = tf.matmul(input_data, w) + b\n",
    "    mu = tf.Variable(0.0, tf.float32)\n",
    "    ra_sigma = tf.Variable(1.0, tf.float32)\n",
    "    a = normalize_batch(a, mu, ra_sigma)\n",
    "    return act(a), a\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "##define placeholder\n",
    "## all image are 28 * 28 so x has 784 dimension\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder')\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "\n",
    "\n",
    "##just some config for not getting whole server\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "sess = tf.Session(config = config)\n",
    "for std in [0.1, 1, 10, 20]:\n",
    "    ## the layers\n",
    "    h, a1 = normalizedFullLayer(X, hidden_size, tf.nn.sigmoid, std)\n",
    "    logits, _ = normalizedFullLayer(h, 10, tf.nn.sigmoid, std)\n",
    "\n",
    "    ## defining loss function\n",
    "    ## use cross entropy of softmax of logits as the loss function\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
    "    ## computes the mean over all the examples in the batch\n",
    "    loss = tf.reduce_mean(entropy) \n",
    "    # + regulation_rate*tf.nn.l2_loss(w)  \n",
    "\n",
    "    ##defining optimizer\n",
    "    ## using gradient descent with learning rate of 0.5 to minimize loss\n",
    "    gradient = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    optimizer = gradient.minimize(loss)\n",
    "\n",
    "    ##the prediction we made\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    ##check how many of them are correct arg maxx is used because Y is one hat\n",
    "    correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "    ## to visualize using TensorBoard\n",
    "    #writer = tf.summary.FileWriter('./graphs/mnist/c', sess.graph)\n",
    "    ####cross validation\n",
    "    ##starting time\n",
    "    start_time = time.time()\n",
    "    ##initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "    n_batches = int(60000 / batch_size)\n",
    "    for i in range(n_epochs):  # train the model n_epochs times\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "\n",
    "        conc = list(zip(train_images, train_labels))\n",
    "        random.shuffle(conc)\n",
    "        train_images, train_labels = zip(*conc)\n",
    "\n",
    "        for j in range(n_batches):\n",
    "            X_batch, Y_batch = train_images[j* batch_size:(j+1)*batch_size], train_labels[j* batch_size:(j+1)*batch_size]\n",
    "            ##training batches\n",
    "            _, loss_batch, acc_batch, test = sess.run([optimizer, loss, accuracy, a1], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            total_loss += loss_batch\n",
    "            total_acc += acc_batch\n",
    "            ## they said that show every 20 step but that would be too much, so i show every 200 step\n",
    "            if j % 200 == 199:\n",
    "                print('step {}, with std_dev {}, Average loss : {}, Accuracy : {:.6f}'.format(i * n_batches + j, std, total_loss / 200, total_acc / batch_size / 200))\n",
    "#                 print(np.mean(np.absolute(test)))\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "    print('Optimization Finished!')\n",
    "\n",
    "    # test the model\n",
    "    ##number of test batches\n",
    "    n_batches = int(10000 / batch_size)\n",
    "    total_correct_preds = 0\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        ##test batches\n",
    "        X_batch, Y_batch = test_images[i* batch_size:(i+1)*batch_size], test_labels[i* batch_size:(i+1)*batch_size]\n",
    "        accuracy_batch = sess.run([accuracy], feed_dict={X: X_batch, Y: Y_batch})\n",
    "        total_correct_preds += accuracy_batch[0]\n",
    "\n",
    "    print('Accuracy {}, with std_dev {}'.format(total_correct_preds / 10000, std))\n",
    "\n",
    "sess.close()\n",
    "'''\n",
    "در سوال 5.1 همانطور که میبینید بعد از نرمالایز کردن ورودی اکیوریسی هم تست هم ترین از حدود 82 به 88 می رسد\n",
    "در قسمت 5.2 همانطور که در ریزالت ها  میتوانید ببینید با انحراف معیار مناسب با بچ نرمالیزیشن به اکیوریسی 96 درصد هم شبکه میرسد\n",
    "نتیجه ای که گرفتیم این بود با زیاد شدن لرنینگ ریت شبکه ی چند لایه همانطور که انتظار میرفت اکیوریسی اش خیلی پایین است زیرا وقتی لرنینگ ریت که خیلی زیاد است دور لاس این ور آن ور میرود\n",
    "ولی همانطور که در سوال های تطوری دیده بودیم شبکه ی با بچ نورمالیزیشن نسبت به لرنینگ ریت طیاد حساس نیست و میتوان لرنینگ ریت زیاد داد\n",
    "اما جفت شیکه وقتی انحراف معیار وزن ها زیاد می شود مقدار اولیه اندازه خود وزن ها زیاد می شود و اکسپلود گرادیان رخ می دهد \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
