{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (10000, 784)\n",
      "mean 33.7912244898\n",
      "max 255\n",
      "min 0\n",
      "shape (60000, 784)\n",
      "mean 33.3184214498\n",
      "max 255\n",
      "min 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from load_mnist import MNIST\n",
    "\n",
    "train_images, train_labels=MNIST(path=\"../Ehsan/Dataset/HW2/Q5\",return_type=\"numpy\",mode=\"vanilla\").load_training()\n",
    "test_images, test_labels=MNIST(path=\"../Ehsan/Dataset/HW2/Q5\",return_type=\"numpy\",mode=\"vanilla\").load_testing()\n",
    "\n",
    "for data in [test_images,train_images]:\n",
    "    print(\"shape\",data.shape)\n",
    "    print(\"mean\",data.mean())\n",
    "    print(\"max\",data.max())\n",
    "    print(\"min\",data.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_one_hat(y):\n",
    "    o = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        o[i][y[i] - 1] = 1\n",
    "    return o\n",
    "\n",
    "test_labels = to_one_hat(test_labels)\n",
    "train_labels = to_one_hat(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 199, with std 0.1, Average loss : 2.295365102291107, Accuracy : 0.133667\n",
      "step 399, with std 0.1, Average loss : 2.183388706445694, Accuracy : 0.365417\n",
      "step 599, with std 0.1, Average loss : 1.9598277503252028, Accuracy : 0.535333\n",
      "step 799, with std 0.1, Average loss : 1.8422295576334, Accuracy : 0.593167\n",
      "step 999, with std 0.1, Average loss : 1.7974943262338638, Accuracy : 0.583250\n",
      "step 1199, with std 0.1, Average loss : 1.7824984139204025, Accuracy : 0.567000\n",
      "step 1399, with std 0.1, Average loss : 1.7788734871149063, Accuracy : 0.574500\n",
      "step 1599, with std 0.1, Average loss : 1.7663910472393036, Accuracy : 0.541083\n",
      "step 1799, with std 0.1, Average loss : 1.7622174525260925, Accuracy : 0.532917\n",
      "step 1999, with std 0.1, Average loss : 1.7550599414110184, Accuracy : 0.529333\n",
      "step 2199, with std 0.1, Average loss : 1.7568813240528107, Accuracy : 0.523833\n",
      "step 2399, with std 0.1, Average loss : 1.766355848312378, Accuracy : 0.508917\n",
      "step 2599, with std 0.1, Average loss : 1.765592126250267, Accuracy : 0.524917\n",
      "step 2799, with std 0.1, Average loss : 1.7782229328155517, Accuracy : 0.520333\n",
      "step 2999, with std 0.1, Average loss : 1.757656859755516, Accuracy : 0.508917\n",
      "step 3199, with std 0.1, Average loss : 1.751386638879776, Accuracy : 0.516583\n",
      "step 3399, with std 0.1, Average loss : 1.7400378984212876, Accuracy : 0.541333\n",
      "step 3599, with std 0.1, Average loss : 1.7479662543535233, Accuracy : 0.500833\n",
      "step 3799, with std 0.1, Average loss : 1.7425241547822952, Accuracy : 0.514250\n",
      "step 3999, with std 0.1, Average loss : 1.72424147605896, Accuracy : 0.520333\n",
      "step 4199, with std 0.1, Average loss : 1.725562145113945, Accuracy : 0.527750\n",
      "step 4399, with std 0.1, Average loss : 1.7314417845010757, Accuracy : 0.529000\n",
      "step 4599, with std 0.1, Average loss : 1.7329396307468414, Accuracy : 0.522417\n",
      "step 4799, with std 0.1, Average loss : 1.7353822660446168, Accuracy : 0.536250\n",
      "step 4999, with std 0.1, Average loss : 1.731163523197174, Accuracy : 0.553833\n",
      "Total time: 32.621070861816406 seconds\n",
      "Optimization Finished!\n",
      "with std 0.1, Accuracy 0.5376\n",
      "step 199, with std 1, Average loss : 2.1908871364593505, Accuracy : 0.105667\n",
      "step 399, with std 1, Average loss : 2.0249305212497712, Accuracy : 0.129583\n",
      "step 599, with std 1, Average loss : 1.91758369743824, Accuracy : 0.431083\n",
      "step 799, with std 1, Average loss : 1.861308572292328, Accuracy : 0.551250\n",
      "step 999, with std 1, Average loss : 1.8303667813539506, Accuracy : 0.592333\n",
      "step 1199, with std 1, Average loss : 1.7845234948396682, Accuracy : 0.645500\n",
      "step 1399, with std 1, Average loss : 1.7721769785881043, Accuracy : 0.661167\n",
      "step 1599, with std 1, Average loss : 1.7752562606334685, Accuracy : 0.646250\n",
      "step 1799, with std 1, Average loss : 1.7713896816968917, Accuracy : 0.661167\n",
      "step 1999, with std 1, Average loss : 1.7659494930505752, Accuracy : 0.669750\n",
      "step 2199, with std 1, Average loss : 1.7559853553771974, Accuracy : 0.682333\n",
      "step 2399, with std 1, Average loss : 1.748476665019989, Accuracy : 0.685333\n",
      "step 2599, with std 1, Average loss : 1.745189641714096, Accuracy : 0.688750\n",
      "step 2799, with std 1, Average loss : 1.7519815754890442, Accuracy : 0.678917\n",
      "step 2999, with std 1, Average loss : 1.7451996099948883, Accuracy : 0.691083\n",
      "step 3199, with std 1, Average loss : 1.7409586501121521, Accuracy : 0.685833\n",
      "step 3399, with std 1, Average loss : 1.732233475446701, Accuracy : 0.699750\n",
      "step 3599, with std 1, Average loss : 1.735096538066864, Accuracy : 0.693083\n",
      "step 3799, with std 1, Average loss : 1.7240791839361191, Accuracy : 0.699500\n",
      "step 3999, with std 1, Average loss : 1.7071760779619216, Accuracy : 0.716667\n",
      "step 4199, with std 1, Average loss : 1.700695322751999, Accuracy : 0.728167\n",
      "step 4399, with std 1, Average loss : 1.7002532309293747, Accuracy : 0.726250\n",
      "step 4599, with std 1, Average loss : 1.6848204958438873, Accuracy : 0.744833\n",
      "step 4799, with std 1, Average loss : 1.6682381862401963, Accuracy : 0.764667\n",
      "step 4999, with std 1, Average loss : 1.6745987302064895, Accuracy : 0.752000\n",
      "Total time: 32.607895851135254 seconds\n",
      "Optimization Finished!\n",
      "with std 1, Accuracy 0.7675\n",
      "step 199, with std 10, Average loss : 2.393245340585709, Accuracy : 0.104167\n",
      "step 399, with std 10, Average loss : 2.374623283147812, Accuracy : 0.108417\n",
      "step 599, with std 10, Average loss : 2.384396951198578, Accuracy : 0.114500\n",
      "step 799, with std 10, Average loss : 2.3805888724327087, Accuracy : 0.111333\n",
      "step 999, with std 10, Average loss : 2.376554844379425, Accuracy : 0.112750\n",
      "step 1199, with std 10, Average loss : 2.377481435537338, Accuracy : 0.119167\n",
      "step 1399, with std 10, Average loss : 2.3692917084693907, Accuracy : 0.121750\n",
      "step 1599, with std 10, Average loss : 2.36816876411438, Accuracy : 0.117417\n",
      "step 1799, with std 10, Average loss : 2.3751633512973784, Accuracy : 0.116750\n",
      "step 1999, with std 10, Average loss : 2.376131454706192, Accuracy : 0.109000\n",
      "step 2199, with std 10, Average loss : 2.3742803275585174, Accuracy : 0.115833\n",
      "step 2399, with std 10, Average loss : 2.3740125131607055, Accuracy : 0.115583\n",
      "step 2599, with std 10, Average loss : 2.367573078870773, Accuracy : 0.123167\n",
      "step 2799, with std 10, Average loss : 2.3680156660079956, Accuracy : 0.127333\n",
      "step 2999, with std 10, Average loss : 2.3742874097824096, Accuracy : 0.124167\n",
      "step 3199, with std 10, Average loss : 2.3752308821678163, Accuracy : 0.121833\n",
      "step 3399, with std 10, Average loss : 2.370527353286743, Accuracy : 0.125250\n",
      "step 3599, with std 10, Average loss : 2.3682219445705415, Accuracy : 0.128917\n",
      "step 3799, with std 10, Average loss : 2.3659138762950898, Accuracy : 0.135167\n",
      "step 3999, with std 10, Average loss : 2.3607305550575255, Accuracy : 0.130917\n",
      "step 4199, with std 10, Average loss : 2.363271872997284, Accuracy : 0.132083\n",
      "step 4399, with std 10, Average loss : 2.3672357058525084, Accuracy : 0.130750\n",
      "step 4599, with std 10, Average loss : 2.370301786661148, Accuracy : 0.137500\n",
      "step 4799, with std 10, Average loss : 2.3681452465057373, Accuracy : 0.128667\n",
      "step 4999, with std 10, Average loss : 2.370409450531006, Accuracy : 0.135917\n",
      "Total time: 32.63684892654419 seconds\n",
      "Optimization Finished!\n",
      "with std 10, Accuracy 0.132\n",
      "step 199, with std 20, Average loss : 2.3947631871700286, Accuracy : 0.120667\n",
      "step 399, with std 20, Average loss : 2.3856899976730346, Accuracy : 0.122167\n",
      "step 599, with std 20, Average loss : 2.403858969211578, Accuracy : 0.117000\n",
      "step 799, with std 20, Average loss : 2.3934182226657867, Accuracy : 0.122000\n",
      "step 999, with std 20, Average loss : 2.3924123525619505, Accuracy : 0.117583\n",
      "step 1199, with std 20, Average loss : 2.387814530134201, Accuracy : 0.120583\n",
      "step 1399, with std 20, Average loss : 2.3910069394111635, Accuracy : 0.122750\n",
      "step 1599, with std 20, Average loss : 2.389520288705826, Accuracy : 0.123833\n",
      "step 1799, with std 20, Average loss : 2.3885881984233857, Accuracy : 0.119333\n",
      "step 1999, with std 20, Average loss : 2.3794077599048613, Accuracy : 0.123417\n",
      "step 2199, with std 20, Average loss : 2.3816858994960786, Accuracy : 0.117667\n",
      "step 2399, with std 20, Average loss : 2.3839208817481996, Accuracy : 0.122833\n",
      "step 2599, with std 20, Average loss : 2.38441223025322, Accuracy : 0.126583\n",
      "step 2799, with std 20, Average loss : 2.3794295167922974, Accuracy : 0.123250\n",
      "step 2999, with std 20, Average loss : 2.3877699518203737, Accuracy : 0.117917\n",
      "step 3199, with std 20, Average loss : 2.374254921674728, Accuracy : 0.122333\n",
      "step 3399, with std 20, Average loss : 2.3868003737926484, Accuracy : 0.121250\n",
      "step 3599, with std 20, Average loss : 2.385013531446457, Accuracy : 0.125917\n",
      "step 3799, with std 20, Average loss : 2.377168347835541, Accuracy : 0.119583\n",
      "step 3999, with std 20, Average loss : 2.386478966474533, Accuracy : 0.120583\n",
      "step 4199, with std 20, Average loss : 2.372902672290802, Accuracy : 0.120750\n",
      "step 4399, with std 20, Average loss : 2.381294286251068, Accuracy : 0.120917\n",
      "step 4599, with std 20, Average loss : 2.381433357000351, Accuracy : 0.124250\n",
      "step 4799, with std 20, Average loss : 2.390620552301407, Accuracy : 0.119833\n",
      "step 4999, with std 20, Average loss : 2.3755877888202668, Accuracy : 0.127083\n",
      "Total time: 32.966052770614624 seconds\n",
      "Optimization Finished!\n",
      "with std 20, Accuracy 0.1257\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "## Define paramaters for the model\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 60 \n",
    "hidden_size = 100\n",
    "learning_rate = tf.Variable(0.5)\n",
    "std_dev = tf.Variable(10.0)\n",
    "# regulation_rate = 1e-4\n",
    "\n",
    "def fullLayer(input_data, output_size, act, std):    \n",
    "    ##defining the full linear Layer here\n",
    "    w = tf.Variable(tf.random_normal([input_data.get_shape().as_list()[1], output_size],mean = 0.0, stddev = std))\n",
    "    b = tf.Variable(tf.random_normal([output_size], stddev = std))\n",
    "    return act(tf.matmul(input_data, w) + b)\n",
    "\n",
    "##define placeholder\n",
    "## all image are 28 * 28 so x has 784 dimension\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder')\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for std in [0.1, 1, 10, 20]:\n",
    "    \n",
    "\n",
    "    ## the layers\n",
    "    layer1 = fullLayer(X, hidden_size, tf.nn.sigmoid, std)\n",
    "    layer2 = fullLayer(layer1, hidden_size, tf.nn.sigmoid, std)\n",
    "    layer3 = fullLayer(layer2, hidden_size, tf.nn.sigmoid, std)\n",
    "    logits = fullLayer(layer3, 10, tf.nn.sigmoid, std)\n",
    "\n",
    "    ## defining loss function\n",
    "    ## use cross entropy of softmax of logits as the loss function\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
    "    ## computes the mean over all the examples in the batch\n",
    "    loss = tf.reduce_mean(entropy) \n",
    "    # + regulation_rate*tf.nn.l2_loss(w)  \n",
    "\n",
    "    ##defining optimizer\n",
    "    ## using gradient descent with learning rate of 0.5 to minimize loss\n",
    "    gradient = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    optimizer = gradient.minimize(loss)\n",
    "\n",
    "    ##the prediction we made\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    ##check how many of them are correct arg maxx is used because Y is one hat\n",
    "    correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "    best_acc = 0\n",
    "    best_hidden = 0\n",
    "\n",
    "    ##just some config for not getting whole server\n",
    "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "    config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "    sess = tf.Session(config = config)\n",
    "    ## to visualize using TensorBoard\n",
    "    #writer = tf.summary.FileWriter('./graphs/mnist/c', sess.graph)\n",
    "    ##starting time\n",
    "    start_time = time.time()\n",
    "    ##initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    n_batches = int(60000 / batch_size)\n",
    "    for i in range(n_epochs):  # train the model n_epochs times\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "\n",
    "        conc = list(zip(train_images, train_labels))\n",
    "        random.shuffle(conc)\n",
    "        train_images, train_labels = zip(*conc)\n",
    "\n",
    "        for j in range(n_batches):\n",
    "            X_batch, Y_batch = train_images[j* batch_size:(j+1)*batch_size], train_labels[j* batch_size:(j+1)*batch_size]\n",
    "            ##training batches\n",
    "            _, loss_batch, acc_batch = sess.run([optimizer, loss, accuracy], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            total_loss += loss_batch\n",
    "            total_acc += acc_batch\n",
    "            if j % 200 == 199:\n",
    "                print('step {}, with std {}, Average loss : {}, Accuracy : {:.6f}'.format(i * n_batches + j, std, total_loss / 200, total_acc / batch_size / 200))\n",
    "                \n",
    "                \n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "    print('Optimization Finished!')\n",
    "\n",
    "    # test the model\n",
    "    ##number of test batches\n",
    "    n_batches = int(10000 / batch_size)\n",
    "    total_correct_preds = 0\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        ##test batches\n",
    "        X_batch, Y_batch = test_images[i* batch_size:(i+1)*batch_size], test_labels[i* batch_size:(i+1)*batch_size]\n",
    "        accuracy_batch = sess.run([accuracy], feed_dict={X: X_batch, Y: Y_batch})\n",
    "        total_correct_preds += accuracy_batch[0]\n",
    "\n",
    "    print('with std {}, Accuracy {}'.format(std, total_correct_preds / 10000))\n",
    "    \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
