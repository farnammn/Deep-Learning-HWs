{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''TensorFlow implementation of http://arxiv.org/pdf/1511.06434.pdf'''\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import prettytensor as pt\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imsave\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from deconv import deconv2d\n",
    "from progressbar import ETA, Bar, Percentage, ProgressBar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.4/dist-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Enc. loss 803.879305, Gen. loss 804.387509, Disc. loss: 1.507158\n",
      "Enc. loss 774.023967, Gen. loss 775.041240, Disc. loss: 0.999417\n",
      "Enc. loss 769.613275, Gen. loss 770.703165, Disc. loss: 0.896076\n",
      "Enc. loss 767.626578, Gen. loss 768.692595, Disc. loss: 0.872468\n",
      "Enc. loss 766.975410, Gen. loss 767.945825, Disc. loss: 0.856333\n",
      "Enc. loss 768.921270, Gen. loss 770.266882, Disc. loss: 0.806690\n",
      "Enc. loss 802.349322, Gen. loss 805.892081, Disc. loss: 0.462065\n",
      "Enc. loss 781.010057, Gen. loss 782.670421, Disc. loss: 0.527099\n",
      "Enc. loss 776.882631, Gen. loss 778.376441, Disc. loss: 0.628903\n",
      "Enc. loss 775.059699, Gen. loss 776.426177, Disc. loss: 0.673996\n",
      "Enc. loss 774.729041, Gen. loss 776.069521, Disc. loss: 0.695186\n",
      "Enc. loss 775.496771, Gen. loss 777.018863, Disc. loss: 0.661639\n",
      "Enc. loss 775.911932, Gen. loss 777.457560, Disc. loss: 0.653730\n",
      "Enc. loss 787.404597, Gen. loss 789.583966, Disc. loss: 0.526324\n",
      "Enc. loss 779.776802, Gen. loss 781.343024, Disc. loss: 0.579378\n",
      "Enc. loss 777.321006, Gen. loss 778.809447, Disc. loss: 0.653957\n",
      "Enc. loss 779.681906, Gen. loss 781.437298, Disc. loss: 0.672952\n",
      "Enc. loss 791.499091, Gen. loss 793.789319, Disc. loss: 0.518147\n",
      "Enc. loss 780.558107, Gen. loss 782.190471, Disc. loss: 0.636257\n",
      "Enc. loss 780.180302, Gen. loss 781.751076, Disc. loss: 0.642973\n",
      "Enc. loss 786.124313, Gen. loss 788.208441, Disc. loss: 0.616057\n",
      "Enc. loss 785.985538, Gen. loss 787.545323, Disc. loss: 0.641375\n",
      "Enc. loss 781.183813, Gen. loss 782.721636, Disc. loss: 0.627329\n",
      "Enc. loss 779.701600, Gen. loss 781.113704, Disc. loss: 0.699253\n",
      "Enc. loss 778.721544, Gen. loss 780.081976, Disc. loss: 0.740608\n",
      "Enc. loss 776.652973, Gen. loss 777.942365, Disc. loss: 0.792394\n",
      "Enc. loss 785.812373, Gen. loss 787.597495, Disc. loss: 0.688074\n",
      "Enc. loss 779.619109, Gen. loss 781.086431, Disc. loss: 0.718842\n",
      "Enc. loss 777.936016, Gen. loss 779.541449, Disc. loss: 0.774195\n",
      "Enc. loss 784.483461, Gen. loss 786.515091, Disc. loss: 0.674905\n",
      "Enc. loss 779.120320, Gen. loss 780.873120, Disc. loss: 0.747725\n",
      "Enc. loss 778.854541, Gen. loss 780.440664, Disc. loss: 0.762334\n",
      "Enc. loss 781.779627, Gen. loss 783.610797, Disc. loss: 0.749814\n",
      "Enc. loss 777.385385, Gen. loss 778.889366, Disc. loss: 0.767455\n",
      "Enc. loss 778.028420, Gen. loss 779.524811, Disc. loss: 0.786948\n",
      "Enc. loss 776.858228, Gen. loss 778.311556, Disc. loss: 0.846756\n",
      "Enc. loss 776.458262, Gen. loss 777.850177, Disc. loss: 0.785359\n",
      "Enc. loss 781.768854, Gen. loss 783.657984, Disc. loss: 0.756724\n",
      "Enc. loss 776.810007, Gen. loss 778.249244, Disc. loss: 0.776897\n",
      "Enc. loss 775.695189, Gen. loss 777.202267, Disc. loss: 0.843932\n",
      "Enc. loss 777.022587, Gen. loss 778.506867, Disc. loss: 0.815259\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "flags = tf.flags\n",
    "logging = tf.logging\n",
    "\n",
    "flags.DEFINE_integer(\"batch_size\", 256, \"batch size\")\n",
    "flags.DEFINE_integer(\"updates_per_epoch\", 100, \"number of updates per epoch\")\n",
    "flags.DEFINE_integer(\"max_epoch\", 100, \"max epoch\")\n",
    "flags.DEFINE_float(\"g_learning_rate\", 1e-2, \"learning rate\")\n",
    "flags.DEFINE_float(\"d_learning_rate\", 1e-3, \"learning rate\")\n",
    "flags.DEFINE_string(\"working_directory\", \"\", \"\")\n",
    "flags.DEFINE_float(\"hidden_size\", 128, \"hidden size\")\n",
    "flags.DEFINE_float(\"gamma\", 1, \"gamma\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def encoder(input_tensor):\n",
    "    '''Create encoder network.\n",
    "        \n",
    "        Args:\n",
    "        input_tensor: a batch of flattened images [batch_size, 28*28]\n",
    "        \n",
    "        Returns:\n",
    "        A tensor that expresses the encoder network\n",
    "        '''\n",
    "    return (pt.wrap(input_tensor).\n",
    "            reshape([FLAGS.batch_size, 28, 28, 1]).\n",
    "            conv2d(5, 32, stride=2).\n",
    "            conv2d(5, 64, stride=2).\n",
    "            conv2d(5, 128, edges='VALID').\n",
    "            dropout(0.9).\n",
    "            flatten().\n",
    "            fully_connected(FLAGS.hidden_size * 2, activation_fn=None)).tensor\n",
    "\n",
    "\n",
    "def discriminator(input_features):\n",
    "    '''Create a network that discriminates between images from a dataset and\n",
    "    generated ones.\n",
    "    Args:\n",
    "        input: a batch of real images [batch, height, width, channels]\n",
    "    Returns:\n",
    "        A tensor that represents the network\n",
    "    '''\n",
    "    return  input_features.fully_connected(1, activation_fn=None).tensor\n",
    "\n",
    "\n",
    "def discriminator_features(input_tensor):\n",
    "    return (pt.wrap(input_tensor).\n",
    "            reshape([FLAGS.batch_size, 28, 28, 1]).\n",
    "            conv2d(5, 32, stride=2).\n",
    "            conv2d(5, 64, stride=2).\n",
    "            conv2d(5, 128, edges='VALID').\n",
    "            dropout(0.9).\n",
    "            flatten())\n",
    "\n",
    "\n",
    "\n",
    "def get_discrinator_loss(D1, D2):\n",
    "    '''Loss for the discriminator network\n",
    "    Args:\n",
    "        D1: logits computed with a discriminator networks from real images\n",
    "        D2: logits computed with a discriminator networks from generated images\n",
    "    Returns:\n",
    "        Cross entropy loss, positive samples have implicit labels 1, negative 0s\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.nn.relu(D1) - D1 + tf.log(1.0 + tf.exp(-tf.abs(D1)))) + \\\n",
    "        tf.reduce_mean(tf.nn.relu(D2) + tf.log(1.0 + tf.exp(-tf.abs(D2))))\n",
    "\n",
    "\n",
    "\n",
    "def generator(input_tensor):\n",
    "    '''Create a network that generates images\n",
    "    TODO: Add fixed initialization, so we can draw interpolated images\n",
    "    Returns:\n",
    "        A deconvolutional (not true deconv, transposed conv2d) network that\n",
    "        generated images.\n",
    "    '''\n",
    "    epsilon = tf.random_normal([FLAGS.batch_size, FLAGS.hidden_size])\n",
    "    mean = input_tensor[:, :FLAGS.hidden_size]\n",
    "    stddev = tf.sqrt(tf.exp(input_tensor[:, FLAGS.hidden_size:]))\n",
    "    input_sample = mean + epsilon * stddev\n",
    "    input_sample = tf.reshape(input_sample, [FLAGS.batch_size, 1, 1, -1])\n",
    "    return (pt.wrap(input_sample).\n",
    "            deconv2d(3, 128, edges='VALID').\n",
    "            deconv2d(5, 64, edges='VALID').\n",
    "            deconv2d(5, 32, stride=2).\n",
    "            deconv2d(5, 1, stride=2, activation_fn=tf.nn.sigmoid)).tensor\n",
    "\n",
    "def binary_crossentropy(t,o):\n",
    "    return -(t*tf.log(o+1e-9) + (1.0-t)*tf.log(1.0-o+1e-9))\n",
    "\n",
    "def get_generator_loss(D2):\n",
    "    '''Loss for the genetor. Maximize probability of generating images that\n",
    "    discrimator cannot differentiate.\n",
    "    Returns:\n",
    "        see the paper\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.nn.relu(D2) - D2 + tf.log(1.0 + tf.exp(-tf.abs(D2))))\n",
    "\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    data_directory = \"../../../MNIST_data\"\n",
    "    mnist = input_data.read_data_sets(data_directory, one_hot=True)\n",
    "\n",
    "    input_tensor = tf.placeholder(tf.float32, [FLAGS.batch_size, 28 * 28])\n",
    "\n",
    "\n",
    "    with pt.defaults_scope(activation_fn=tf.nn.elu,\n",
    "                           batch_normalize=True,\n",
    "                           learned_moments_update_rate=0.0003,\n",
    "                           variance_epsilon=0.001,\n",
    "                           scale_after_normalization=True):\n",
    "            with tf.variable_scope(\"encoder\"):\n",
    "                encoding = encoder(input_tensor)\n",
    "            E_params_num = len(tf.trainable_variables())\n",
    "            with tf.variable_scope(\"model\"):\n",
    "                input_features = discriminator_features(input_tensor)  # positive examples\n",
    "                D1 = discriminator(input_features)\n",
    "                input_features = input_features.tensor\n",
    "                D_params_num = len(tf.trainable_variables())\n",
    "                G = generator(encoding)\n",
    "\n",
    "\n",
    "            with tf.variable_scope(\"model\", reuse=True):\n",
    "                gen_features = discriminator_features(G)  # positive examples\n",
    "                D2 = discriminator(gen_features)\n",
    "                gen_features = gen_features.tensor\n",
    "            \n",
    "\n",
    "                    \n",
    "    reconstruction_loss = binary_crossentropy(tf.sigmoid(input_features), tf.sigmoid(gen_features))\n",
    "    reconstruction_loss = tf.reduce_mean(tf.reduce_sum(reconstruction_loss, 1))\n",
    "    D_loss = get_discrinator_loss(D1, D2)\n",
    "    G_loss = FLAGS.gamma * reconstruction_loss + get_generator_loss(D2)\n",
    "\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, epsilon=1.0)\n",
    "    params = tf.trainable_variables()\n",
    "    E_params = params[:E_params_num]\n",
    "    D_params = params[E_params_num:D_params_num]\n",
    "    G_params = params[D_params_num:]\n",
    "#    train_discrimator = optimizer.minimize(loss=D_loss, var_list=D_params)\n",
    "#    train_generator = optimizer.minimize(loss=G_loss, var_list=G_params)\n",
    "    train_encoder = pt.apply_optimizer(optimizer, losses=[reconstruction_loss], regularize=True, include_marked=True, var_list=E_params)\n",
    "    train_discrimator = pt.apply_optimizer(optimizer, losses=[D_loss], regularize=True, include_marked=True, var_list=D_params)\n",
    "    train_generator = pt.apply_optimizer(optimizer, losses=[G_loss], regularize=True, include_marked=True, var_list=G_params)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "    config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "    with tf.Session(config = config) as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(FLAGS.max_epoch):\n",
    "\n",
    "            discriminator_loss = 0.0\n",
    "            generator_loss = 0.0\n",
    "            encoder_loss = 0.0\n",
    "\n",
    "            widgets = [\"epoch #%d|\" % epoch, Percentage(), Bar(), ETA()]\n",
    "#             pbar = ProgressBar(FLAGS.updates_per_epoch, widgets=widgets)\n",
    "#             pbar.start()\n",
    "            for i in range(FLAGS.updates_per_epoch):\n",
    "#                 pbar.update(i)\n",
    "                x, _ = mnist.train.next_batch(FLAGS.batch_size)\n",
    "                \n",
    "                _, loss_value = sess.run([train_encoder, reconstruction_loss], {input_tensor: x, learning_rate: FLAGS.g_learning_rate})\n",
    "                \n",
    "                encoder_loss += loss_value\n",
    "                \n",
    "                _, loss_value = sess.run([train_discrimator, D_loss], {input_tensor: x, learning_rate: FLAGS.d_learning_rate})\n",
    "                discriminator_loss += loss_value\n",
    "\n",
    "                # We still need input for moving averages.\n",
    "                # Need to find how to fix it.\n",
    "                _, loss_value, imgs = sess.run([train_generator, G_loss, G], {input_tensor: x, learning_rate: FLAGS.g_learning_rate})\n",
    "                generator_loss += loss_value\n",
    "\n",
    "            discriminator_loss = discriminator_loss / FLAGS.updates_per_epoch\n",
    "            generator_loss = generator_loss / FLAGS.updates_per_epoch\n",
    "            encoder_loss = encoder_loss / FLAGS.updates_per_epoch\n",
    "\n",
    "            print(\"Enc. loss %f, Gen. loss %f, Disc. loss: %f\" % (encoder_loss, generator_loss,\n",
    "                                                    discriminator_loss))\n",
    "\n",
    "            for k in range(FLAGS.batch_size):\n",
    "                imgs_folder = os.path.join(FLAGS.working_directory, 'imgs')\n",
    "                if not os.path.exists(imgs_folder):\n",
    "                    os.makedirs(imgs_folder)\n",
    "\n",
    "                imsave(os.path.join(imgs_folder, '%d.png') % k,\n",
    "                       imgs[k].reshape(28, 28))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
