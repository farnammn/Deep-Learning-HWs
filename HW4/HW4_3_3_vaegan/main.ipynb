{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''TensorFlow implementation of http://arxiv.org/pdf/1511.06434.pdf'''\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import prettytensor as pt\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imsave\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from deconv import deconv2d\n",
    "from progressbar import ETA, Bar, Percentage, ProgressBar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Enc. loss 794.650623, Gen. loss 795.065546, Disc. loss: 1.460629\n",
      "Enc. loss 754.255105, Gen. loss 755.258925, Disc. loss: 1.029761\n",
      "Enc. loss 747.962747, Gen. loss 749.035913, Disc. loss: 1.024561\n",
      "Enc. loss 753.804984, Gen. loss 755.126761, Disc. loss: 0.995950\n",
      "Enc. loss 751.132876, Gen. loss 752.098077, Disc. loss: 0.980576\n",
      "Enc. loss 746.106672, Gen. loss 747.108070, Disc. loss: 1.078193\n",
      "Enc. loss 745.106258, Gen. loss 746.013362, Disc. loss: 1.117028\n",
      "Enc. loss 744.655734, Gen. loss 745.597807, Disc. loss: 1.146365\n",
      "Enc. loss 744.478962, Gen. loss 745.332519, Disc. loss: 1.157262\n",
      "Enc. loss 744.356262, Gen. loss 745.218860, Disc. loss: 1.166742\n",
      "Enc. loss 744.381372, Gen. loss 745.292422, Disc. loss: 1.174168\n",
      "Enc. loss 745.007578, Gen. loss 745.881351, Disc. loss: 1.161295\n",
      "Enc. loss 755.969777, Gen. loss 757.632064, Disc. loss: 1.035142\n",
      "Enc. loss 753.432869, Gen. loss 755.052557, Disc. loss: 0.956181\n",
      "Enc. loss 749.380230, Gen. loss 750.437087, Disc. loss: 1.025515\n",
      "Enc. loss 747.389641, Gen. loss 748.338247, Disc. loss: 1.089300\n",
      "Enc. loss 746.974960, Gen. loss 747.915717, Disc. loss: 1.115228\n",
      "Enc. loss 746.884451, Gen. loss 747.791405, Disc. loss: 1.127320\n",
      "Enc. loss 746.905090, Gen. loss 747.835640, Disc. loss: 1.145525\n",
      "Enc. loss 747.483790, Gen. loss 748.585674, Disc. loss: 1.126961\n",
      "Enc. loss 760.515866, Gen. loss 762.459160, Disc. loss: 0.905977\n",
      "Enc. loss 749.887600, Gen. loss 750.869222, Disc. loss: 1.074124\n",
      "Enc. loss 748.486256, Gen. loss 749.484379, Disc. loss: 1.128639\n",
      "Enc. loss 748.026566, Gen. loss 748.941516, Disc. loss: 1.134443\n",
      "Enc. loss 747.762521, Gen. loss 748.718088, Disc. loss: 1.152273\n",
      "Enc. loss 747.663669, Gen. loss 748.572473, Disc. loss: 1.178825\n",
      "Enc. loss 747.394501, Gen. loss 748.287781, Disc. loss: 1.177746\n",
      "Enc. loss 761.864853, Gen. loss 765.243927, Disc. loss: 1.049339\n",
      "Enc. loss 759.864676, Gen. loss 761.624021, Disc. loss: 1.022965\n",
      "Enc. loss 750.477917, Gen. loss 751.485607, Disc. loss: 1.059447\n",
      "Enc. loss 748.928189, Gen. loss 749.922797, Disc. loss: 1.089469\n",
      "Enc. loss 748.210990, Gen. loss 749.250554, Disc. loss: 1.137751\n",
      "Enc. loss 747.930978, Gen. loss 748.920474, Disc. loss: 1.159002\n",
      "Enc. loss 747.392454, Gen. loss 748.366503, Disc. loss: 1.193362\n",
      "Enc. loss 747.366385, Gen. loss 748.344485, Disc. loss: 1.177590\n",
      "Enc. loss 747.345759, Gen. loss 748.300226, Disc. loss: 1.194822\n",
      "Enc. loss 747.387169, Gen. loss 748.310676, Disc. loss: 1.227808\n",
      "Enc. loss 747.194131, Gen. loss 748.139691, Disc. loss: 1.208824\n",
      "Enc. loss 747.543213, Gen. loss 748.523965, Disc. loss: 1.204431\n",
      "Enc. loss 746.849017, Gen. loss 747.771781, Disc. loss: 1.221453\n",
      "Enc. loss 746.724072, Gen. loss 747.640801, Disc. loss: 1.237235\n",
      "Enc. loss 747.055310, Gen. loss 748.015207, Disc. loss: 1.224403\n",
      "Enc. loss 751.419958, Gen. loss 752.777781, Disc. loss: 1.167556\n",
      "Enc. loss 750.361068, Gen. loss 751.337221, Disc. loss: 1.118851\n",
      "Enc. loss 747.477456, Gen. loss 748.401041, Disc. loss: 1.179515\n",
      "Enc. loss 746.985886, Gen. loss 747.908015, Disc. loss: 1.203111\n",
      "Enc. loss 746.776427, Gen. loss 747.661165, Disc. loss: 1.212774\n",
      "Enc. loss 746.361736, Gen. loss 747.247564, Disc. loss: 1.254646\n",
      "Enc. loss 746.323230, Gen. loss 747.236465, Disc. loss: 1.237103\n",
      "Enc. loss 746.354050, Gen. loss 747.275008, Disc. loss: 1.250773\n",
      "Enc. loss 746.966110, Gen. loss 747.905991, Disc. loss: 1.239097\n",
      "Enc. loss 746.702637, Gen. loss 747.593093, Disc. loss: 1.252509\n",
      "Enc. loss 746.200243, Gen. loss 747.159709, Disc. loss: 1.255401\n",
      "Enc. loss 746.113336, Gen. loss 746.995635, Disc. loss: 1.254957\n",
      "Enc. loss 746.870573, Gen. loss 747.870822, Disc. loss: 1.250255\n",
      "Enc. loss 748.843829, Gen. loss 749.833013, Disc. loss: 1.243570\n",
      "Enc. loss 746.192759, Gen. loss 747.089057, Disc. loss: 1.248780\n",
      "Enc. loss 746.133029, Gen. loss 747.016820, Disc. loss: 1.224235\n",
      "Enc. loss 746.053795, Gen. loss 746.973518, Disc. loss: 1.236126\n",
      "Enc. loss 745.801544, Gen. loss 746.676053, Disc. loss: 1.278866\n",
      "Enc. loss 745.780073, Gen. loss 746.683995, Disc. loss: 1.248108\n",
      "Enc. loss 745.758972, Gen. loss 746.639241, Disc. loss: 1.267573\n",
      "Enc. loss 746.697307, Gen. loss 747.635872, Disc. loss: 1.239472\n",
      "Enc. loss 745.924846, Gen. loss 746.886774, Disc. loss: 1.293957\n",
      "Enc. loss 745.675035, Gen. loss 746.585811, Disc. loss: 1.253303\n",
      "Enc. loss 745.821739, Gen. loss 746.695982, Disc. loss: 1.264350\n",
      "Enc. loss 752.926977, Gen. loss 754.416524, Disc. loss: 1.296008\n",
      "Enc. loss 747.762684, Gen. loss 748.671187, Disc. loss: 1.234401\n",
      "Enc. loss 746.755020, Gen. loss 747.708168, Disc. loss: 1.199026\n",
      "Enc. loss 746.213809, Gen. loss 747.194507, Disc. loss: 1.202405\n",
      "Enc. loss 746.243445, Gen. loss 747.166615, Disc. loss: 1.201107\n",
      "Enc. loss 746.193725, Gen. loss 747.104176, Disc. loss: 1.195075\n",
      "Enc. loss 746.605306, Gen. loss 747.556743, Disc. loss: 1.204841\n",
      "Enc. loss 746.665825, Gen. loss 747.583750, Disc. loss: 1.232565\n",
      "Enc. loss 746.200704, Gen. loss 747.125854, Disc. loss: 1.204204\n",
      "Enc. loss 746.187173, Gen. loss 747.144459, Disc. loss: 1.255587\n",
      "Enc. loss 746.725428, Gen. loss 747.714841, Disc. loss: 1.255839\n",
      "Enc. loss 745.998318, Gen. loss 746.955735, Disc. loss: 1.238511\n",
      "Enc. loss 745.863703, Gen. loss 746.792675, Disc. loss: 1.209910\n",
      "Enc. loss 745.787509, Gen. loss 746.729427, Disc. loss: 1.250644\n",
      "Enc. loss 747.519770, Gen. loss 748.654451, Disc. loss: 1.194510\n",
      "Enc. loss 752.539434, Gen. loss 753.670178, Disc. loss: 1.239287\n",
      "Enc. loss 747.141604, Gen. loss 748.151735, Disc. loss: 1.160602\n",
      "Enc. loss 746.573531, Gen. loss 747.631171, Disc. loss: 1.154423\n",
      "Enc. loss 746.543018, Gen. loss 747.508131, Disc. loss: 1.150216\n",
      "Enc. loss 746.402314, Gen. loss 747.371265, Disc. loss: 1.200626\n",
      "Enc. loss 746.324149, Gen. loss 747.297421, Disc. loss: 1.205906\n",
      "Enc. loss 750.249935, Gen. loss 751.423463, Disc. loss: 1.182458\n",
      "Enc. loss 753.490431, Gen. loss 754.580186, Disc. loss: 1.121049\n",
      "Enc. loss 747.483546, Gen. loss 748.559235, Disc. loss: 1.121280\n",
      "Enc. loss 747.266077, Gen. loss 748.297654, Disc. loss: 1.123609\n",
      "Enc. loss 746.945735, Gen. loss 747.985202, Disc. loss: 1.123267\n",
      "Enc. loss 746.672125, Gen. loss 747.698448, Disc. loss: 1.159357\n",
      "Enc. loss 746.598271, Gen. loss 747.604527, Disc. loss: 1.201937\n",
      "Enc. loss 746.999707, Gen. loss 747.979084, Disc. loss: 1.117275\n",
      "Enc. loss 746.577526, Gen. loss 747.569620, Disc. loss: 1.208737\n",
      "Enc. loss 746.721376, Gen. loss 747.743581, Disc. loss: 1.179058\n",
      "Enc. loss 746.568517, Gen. loss 747.542147, Disc. loss: 1.196634\n",
      "Enc. loss 746.912936, Gen. loss 747.910092, Disc. loss: 1.181088\n",
      "Enc. loss 750.295281, Gen. loss 751.393190, Disc. loss: 1.152599\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "flags = tf.flags\n",
    "logging = tf.logging\n",
    "\n",
    "flags.DEFINE_integer(\"batch_size\", 256, \"batch size\")\n",
    "flags.DEFINE_integer(\"updates_per_epoch\", 100, \"number of updates per epoch\")\n",
    "flags.DEFINE_integer(\"max_epoch\", 100, \"max epoch\")\n",
    "flags.DEFINE_float(\"g_learning_rate\", 1e-2, \"learning rate\")\n",
    "flags.DEFINE_float(\"d_learning_rate\", 1e-3, \"learning rate\")\n",
    "flags.DEFINE_string(\"working_directory\", \"\", \"\")\n",
    "flags.DEFINE_float(\"hidden_size\", 128, \"hidden size\")\n",
    "flags.DEFINE_float(\"gamma\", 1, \"gamma\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def encoder(input_tensor):\n",
    "    '''Create encoder network.\n",
    "        \n",
    "        Args:\n",
    "        input_tensor: a batch of flattened images [batch_size, 28*28]\n",
    "        \n",
    "        Returns:\n",
    "        A tensor that expresses the encoder network\n",
    "        '''\n",
    "    return (pt.wrap(input_tensor).\n",
    "            reshape([FLAGS.batch_size, 28, 28, 1]).\n",
    "            conv2d(5, 32, stride=2).\n",
    "            conv2d(5, 64, stride=2).\n",
    "            conv2d(5, 128, edges='VALID').\n",
    "            dropout(0.9).\n",
    "            flatten().\n",
    "            fully_connected(FLAGS.hidden_size * 2, activation_fn=None)).tensor\n",
    "\n",
    "\n",
    "def discriminator(input_features):\n",
    "    '''Create a network that discriminates between images from a dataset and\n",
    "    generated ones.\n",
    "    Args:\n",
    "        input: a batch of real images [batch, height, width, channels]\n",
    "    Returns:\n",
    "        A tensor that represents the network\n",
    "    '''\n",
    "    return  input_features.fully_connected(1, activation_fn=None).tensor\n",
    "\n",
    "\n",
    "def discriminator_features(input_tensor):\n",
    "    return (pt.wrap(input_tensor).\n",
    "            reshape([FLAGS.batch_size, 28, 28, 1]).\n",
    "            conv2d(5, 32, stride=2).\n",
    "            conv2d(5, 64, stride=2).\n",
    "            conv2d(5, 128, edges='VALID').\n",
    "            dropout(0.9).\n",
    "            flatten())\n",
    "\n",
    "\n",
    "\n",
    "def get_discrinator_loss(D1, D2):\n",
    "    '''Loss for the discriminator network\n",
    "    Args:\n",
    "        D1: logits computed with a discriminator networks from real images\n",
    "        D2: logits computed with a discriminator networks from generated images\n",
    "    Returns:\n",
    "        Cross entropy loss, positive samples have implicit labels 1, negative 0s\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.nn.relu(D1) - D1 + tf.log(1.0 + tf.exp(-tf.abs(D1)))) + \\\n",
    "        tf.reduce_mean(tf.nn.relu(D2) + tf.log(1.0 + tf.exp(-tf.abs(D2))))\n",
    "\n",
    "\n",
    "\n",
    "def generator(input_tensor):\n",
    "    '''Create a network that generates images\n",
    "    TODO: Add fixed initialization, so we can draw interpolated images\n",
    "    Returns:\n",
    "        A deconvolutional (not true deconv, transposed conv2d) network that\n",
    "        generated images.\n",
    "    '''\n",
    "    epsilon = tf.random_normal([FLAGS.batch_size, FLAGS.hidden_size])\n",
    "    mean = input_tensor[:, :FLAGS.hidden_size]\n",
    "    stddev = tf.sqrt(tf.exp(input_tensor[:, FLAGS.hidden_size:]))\n",
    "    input_sample = mean + epsilon * stddev\n",
    "    input_sample = tf.reshape(input_sample, [FLAGS.batch_size, 1, 1, -1])\n",
    "    return (pt.wrap(input_sample).\n",
    "            deconv2d(3, 128, edges='VALID').\n",
    "            deconv2d(5, 64, edges='VALID').\n",
    "            deconv2d(5, 32, stride=2).\n",
    "            deconv2d(5, 1, stride=2, activation_fn=tf.nn.sigmoid)).tensor\n",
    "\n",
    "def binary_crossentropy(t,o):\n",
    "    return -(t*tf.log(o+1e-9) + (1.0-t)*tf.log(1.0-o+1e-9))\n",
    "\n",
    "def get_generator_loss(D2):\n",
    "    '''Loss for the genetor. Maximize probability of generating images that\n",
    "    discrimator cannot differentiate.\n",
    "    Returns:\n",
    "        see the paper\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.nn.relu(D2) - D2 + tf.log(1.0 + tf.exp(-tf.abs(D2))))\n",
    "\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    data_directory = \"../../../MNIST_data\"\n",
    "    mnist = input_data.read_data_sets(data_directory, one_hot=True)\n",
    "\n",
    "    input_tensor = tf.placeholder(tf.float32, [FLAGS.batch_size, 28 * 28])\n",
    "\n",
    "\n",
    "    with pt.defaults_scope(activation_fn=tf.nn.elu,\n",
    "                           batch_normalize=True,\n",
    "                           learned_moments_update_rate=0.0003,\n",
    "                           variance_epsilon=0.001,\n",
    "                           scale_after_normalization=True):\n",
    "            with tf.variable_scope(\"encoder\"):\n",
    "                encoding = encoder(input_tensor)\n",
    "            E_params_num = len(tf.trainable_variables())\n",
    "            with tf.variable_scope(\"model\"):\n",
    "                input_features = discriminator_features(input_tensor)  # positive examples\n",
    "                D1 = discriminator(input_features)\n",
    "                input_features = input_features.tensor\n",
    "                D_params_num = len(tf.trainable_variables())\n",
    "                G = generator(encoding)\n",
    "\n",
    "\n",
    "            with tf.variable_scope(\"model\", reuse=True):\n",
    "                gen_features = discriminator_features(G)  # positive examples\n",
    "                D2 = discriminator(gen_features)\n",
    "                gen_features = gen_features.tensor\n",
    "            \n",
    "\n",
    "                    \n",
    "    reconstruction_loss = binary_crossentropy(tf.sigmoid(input_features), tf.sigmoid(gen_features))\n",
    "    reconstruction_loss = tf.reduce_mean(tf.reduce_sum(reconstruction_loss, 1))\n",
    "    D_loss = get_discrinator_loss(D1, D2)\n",
    "    G_loss = FLAGS.gamma * reconstruction_loss + get_generator_loss(D2)\n",
    "\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, epsilon=1.0)\n",
    "    params = tf.trainable_variables()\n",
    "    E_params = params[:E_params_num]\n",
    "    D_params = params[E_params_num:D_params_num]\n",
    "    G_params = params[D_params_num:]\n",
    "#    train_discrimator = optimizer.minimize(loss=D_loss, var_list=D_params)\n",
    "#    train_generator = optimizer.minimize(loss=G_loss, var_list=G_params)\n",
    "    train_encoder = pt.apply_optimizer(optimizer, losses=[reconstruction_loss], regularize=True, include_marked=True, var_list=E_params)\n",
    "    train_discrimator = pt.apply_optimizer(optimizer, losses=[D_loss], regularize=True, include_marked=True, var_list=D_params)\n",
    "    train_generator = pt.apply_optimizer(optimizer, losses=[G_loss], regularize=True, include_marked=True, var_list=G_params)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "    config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "    with tf.Session(config = config) as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(FLAGS.max_epoch):\n",
    "\n",
    "            discriminator_loss = 0.0\n",
    "            generator_loss = 0.0\n",
    "            encoder_loss = 0.0\n",
    "\n",
    "            widgets = [\"epoch #%d|\" % epoch, Percentage(), Bar(), ETA()]\n",
    "#             pbar = ProgressBar(FLAGS.updates_per_epoch, widgets=widgets)\n",
    "#             pbar.start()\n",
    "            for i in range(FLAGS.updates_per_epoch):\n",
    "#                 pbar.update(i)\n",
    "                x, _ = mnist.train.next_batch(FLAGS.batch_size)\n",
    "                \n",
    "                _, loss_value = sess.run([train_encoder, reconstruction_loss], {input_tensor: x, learning_rate: FLAGS.g_learning_rate})\n",
    "                \n",
    "                encoder_loss += loss_value\n",
    "                \n",
    "                _, loss_value = sess.run([train_discrimator, D_loss], {input_tensor: x, learning_rate: FLAGS.d_learning_rate})\n",
    "                discriminator_loss += loss_value\n",
    "\n",
    "                # We still need input for moving averages.\n",
    "                # Need to find how to fix it.\n",
    "                _, loss_value, imgs = sess.run([train_generator, G_loss, G], {input_tensor: x, learning_rate: FLAGS.g_learning_rate})\n",
    "                generator_loss += loss_value\n",
    "\n",
    "            discriminator_loss = discriminator_loss / FLAGS.updates_per_epoch\n",
    "            generator_loss = generator_loss / FLAGS.updates_per_epoch\n",
    "            encoder_loss = encoder_loss / FLAGS.updates_per_epoch\n",
    "\n",
    "            print(\"Enc. loss %f, Gen. loss %f, Disc. loss: %f\" % (encoder_loss, generator_loss,\n",
    "                                                    discriminator_loss))\n",
    "\n",
    "            for k in range(FLAGS.batch_size):\n",
    "                imgs_folder = os.path.join(FLAGS.working_directory, 'imgs')\n",
    "                if not os.path.exists(imgs_folder):\n",
    "                    os.makedirs(imgs_folder)\n",
    "\n",
    "                imsave(os.path.join(imgs_folder, '%d.png') % k,\n",
    "                       imgs[k].reshape(28, 28))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
